MM – Feasibility & Validation Dossier

Executive Summary

Manny Manifolds is a proposed continually-learning AI substrate that represents knowledge as a dynamic manifold (graph) with curvature values on nodes/edges encoding strength/valence. Each user query launches a “thread” (path) through this graph (following geodesics shaped by curvature), with a large language model (LLM) only providing semantic embeddings or labels – not controlling the plan. The system learns locally and incrementally: each traversed edge’s curvature is adjusted by a small valence-weighted increment, with decay and periodic “consolidation” (pruning and motif mining) to stabilize memory. The vision is a self-organizing knowledge base that improves with use, reuses substructures (“motifs”) across tasks, and produces human-interpretable explanations via the paths it finds. Manny is also designed for eventual deployment on event-driven neuromorphic or analog hardware, given its sparse, local operations.

Go/No-Go Recommendation: Go (proceed with a focused 90-day MVP) – but with careful risk mitigation. The concept is ambitious and unproven in full, yet key pieces are supported by recent advances in continual learning, sparse neural updates, neuromorphic computing, and graph reasoning interpretability (as detailed below). The potential payoff – a system that learns online without catastrophic forgetting, explains its reasoning, and runs efficiently on novel hardware – is high. However, the greatest risks are stability (avoiding divergent or trivial curvature updates) and utility (ensuring Manny’s explanations and motif-reuse truly boost performance over simpler baselines like LLM+vector databases or standard graph neural nets). Early-stage experiments should serve as falsification trials for these risks. We recommend proceeding with a constrained MVP (single-domain graph, limited nodes) to test the core hypotheses (H1–H5) in simulation. If Manny fails early tests (e.g. curvature “explosions” or no better retention than a baseline), be prepared to pivot or kill the project. Otherwise, a successful MVP demonstrating, say, >20% path-length convergence and reasonable stability would warrant a more extensive follow-on.

Next 90-Day Plan (High-Level):
	•	Month 1: Build the MVP graph engine and diagnostics. Reproduce a simple continual learning scenario (e.g. a toy “Apple→Pear” culinary domain) with Manny’s online updates. Instrument key metrics: path lengths over repeats, curvature variance, motif reuse counts, LLM token usage, etc. Milestone: Basic thread traversal and /why explanation working on a small domain.
	•	Month 2: Run controlled experiments vs. baselines. Compare Manny to (a) an LLM+retrieval agent and (b) a retrain-or-replay graph model on incremental tasks. Evaluate H1–H5 success criteria (see Experiments section). Identify failure modes (e.g. drift, spurious motifs) via ablations (turn off valence, turn off consolidation, etc.). Milestone: Evidence of learning without catastrophic forgetting (or determination that Manny cannot beat naive baselines).
	•	Month 3: Tackle stability and scalability. Implement adaptive plasticity tuning (or meta-learning of update rate) if needed to keep curvature in bounds. Test on a slightly larger or more dynamic dataset (e.g. add new concepts mid-stream) to assess transfer (H3) and explanation alignment (H2). Develop a prototype UI for visualizing paths/curvature to facilitate a qualitative alignment check with human expectations. Milestone: Go/No-go decision point – If Manny shows promise (measurable improvement on retention/transfer with meaningful explanations) and manageable engineering complexity, proceed to a full project. If not, document learnings and consider alternative approaches (e.g. integrating more traditional memory components or using the LLM more directly).

Overall, given the supportive evidence gathered (see below) and the modular nature of the MVP, we suggest a “Go” with caution – validate quickly whether Manny’s unique curvature-based learning yields an advantage. The next sections present detailed findings graded by evidence strength, a risk register, experimental design, prior art mapping, resource estimates, and an ethics review to guide this decision.

Evidence-Graded Findings

H1: Stable Online Learning via Local Updates (Geometry as Memory)

Hypothesis H1: Manny’s local curvature updates (Δκ = η · valence · f(usage), with decay) can enable continuous learning without catastrophic forgetting or drift, especially if combined with periodic consolidation. We expect to see repeated queries converging to shorter geodesic paths (≥20% length reduction) and faster answers over time, without having to pause for full retraining.

Current evidence supports the feasibility of fast, sparse weight updates to learn new information while preserving old knowledge. Recent studies in continual learning suggest that only a tiny fraction of parameters needs to be modified per new data point to integrate it. For example, memory-efficient training methods find that a given task’s knowledge often resides in ~0.01% of weights ￼ (Evidence B), and selectively tuning <5% of a model’s weights with strong sparsity can achieve high plasticity without forgetting. In one experiment, updating only ~0.5% of parameters per batch (by random gradient dropping) still yielded 15–18% accuracy gains on new tasks vs. baseline, outperforming both full fine-tuning and static prompts ￼ ￼ (Evidence B). This aligns with Manny’s design: only a small, usage-weighted subset of edges gets a curvature tweak on each query. Furthermore, the common baseline of experience replay (re-training on past data) is known to be inefficient at scale – “imagine rehearsing all books you’ve ever read every time you learn to code” as one analysis notes ￼ (Evidence C). Manny’s approach avoids such full replays, focusing on minimal local adjustments, which is promising for long-term efficiency.

Critically, stability mechanisms will be needed to prevent runaway changes. Unconstrained Hebbian learning can lead to positive feedback loops and divergence (the classic “catastrophic forgetting” or sudden drifts). Biology and prior models offer a solution: homeostatic plasticity (global or local weight scaling) can rein in unbounded growth. Indeed, neuroscientific models have shown that purely homeostatic rules (slowly adjusting all synapses to maintain firing rates) can by themselves form stable memory “engrams” without explicit Hebbian triggers ￼ ￼ (Evidence A). The Manny MVP implements analogous controls: each message’s total curvature change is budget-capped (e.g. ±0.06 per turn) and individual edge curvatures are clamped within a finite range (e.g. ±1.5) ￼ (Evidence C). Additionally, L1 weight decay and periodic “micro-decay” ensure that unused or low-valence edges gradually revert toward neutral ￼ ￼ (Evidence C). These measures echo known techniques like synaptic scaling and elastic weight consolidation, which add negative feedback to stabilize learning. For instance, Elastic Weight Consolidation (EWC) uses a quadratic penalty to prevent important weights from straying too far; it was shown to preserve prior task performance on simple benchmarks by treating weight importance as a stabilizing force ￼ ￼ (Evidence B). Manny’s curvature-decay could play a similar role, “anchoring” well-used edges by slowly shrinking their updates over time.

In summary, evidence is moderately strong (B) that a local-update, sparsity-focused learning regime can achieve stable online adaptation. Multiple recent CL methods achieve plasticity without catastrophic forgetting by updating only critical weights (or adding tiny modules) per new experience ￼ ￼. Manny’s twist is to implement this at the level of a graph’s edges with an explicit geometric interpretation. The hypothesis will be supported if our MVP shows, e.g., a new query can be answered correctly after a single pass (one-shot learning of that query’s answer path) and older queries remain answerable with minimal degradation – all without a costly replay or reset. It will likely fail if we observe either oscillation/divergence in curvature (needing frequent global re-projection to fix) or if significant forgetting occurs (e.g. edges keep decaying away important info). Early experiments should monitor curvature distributions and performance on held-out old queries after each new interaction to detect any instability.

Key quantitative signposts: a bounded variance in curvature (e.g. no edge’s |κ| growing unchecked, staying ≪ max clamp), geodesic shortening on recurring questions (at least 20% path-length reduction as learning criterion ￼), and retention of prior answers (backward transfer ≥ 0, ideally positive ￼). If these hold without manual resets, H1 is supported.

H2: Explainable Reasoning via Paths & Curvature (Interpretability)

Hypothesis H2: Manny will provide human-understandable explanations for its answers by exposing the paths and curvature values that led to a decision. In other words, when asked “/why”, the system can show the chain of associations (nodes/edges traversed) along with their strengths, and these should align with human logic ≥80% of the time on evaluation (i.e. people agree the path is a sensible justification). The bold claim is that Manny’s explanations are not just post-hoc rationalizations, but intrinsic to its computation (since it literally reasons via graph traversal).

There is promising evidence that path-based reasoning on knowledge graphs yields inherently interpretable predictions. For example, in biomedical knowledge graph tasks, models that consider explicit paths between entities outperform embedding-only methods and provide clear explanations. BioPathNet, a recent GNN based on the Bellman-Ford algorithm, learns representations over all paths between a head and tail node instead of independent node embeddings. As a result, it can visualize influential paths for each prediction, facilitating validation by domain experts ￼ ￼ (Evidence B). The authors note that these path examples “serve as model explanations, highlighting the paths that lead to the model’s predictions” ￼. Similarly, Renaux et al. (2023) construct a heterogeneous knowledge graph for genetic diseases and use an interpretable decision set classifier that picks specific subgraphs (motifs) as explanations. For each predicted disease-causing gene pair, their system outputs a subgraph of the knowledge graph (specific genes, variants, and relationships) that directly explains why that pair was flagged ￼ (Evidence A). These explanations are not post-hoc saliency maps but the actual features used – just as Manny’s thread is the actual route used to derive an answer. In both cases, human experts can inspect the path/subgraph and often validate that it corresponds to a known mechanism or plausible hypothesis, showing high alignment between the model’s reasoning and human reasoning in that domain.

By contrast, most deep learning models (e.g. transformers) require post-hoc methods to explain decisions – attention weight analysis, gradient-based attribution, or probing for “concept vectors.” These can be insightful but often lack guaranteed correlation with the model’s true internals (the attributions can be misleading or incomplete – decorative rather than causative explanations). Manny’s approach avoids this by design: a “why” query literally traces the computation graph it used. Each edge’s curvature can be seen as contributing a certain cost or tendency to the path. This is akin to symbolic reasoning or case-based reasoning (like a classical logic chain or a case retrieved from memory).

However, to truly test H2, we must verify that the shown paths genuinely predict model behavior and are not spurious. One way to pressure-test this is causal intervention: if we remove a highly-curved edge that appears in an explanation, does Manny’s answer change accordingly? Because Manny’s decisions are explicitly tied to edge weights, we expect yes – which would confirm the explanations are faithful. This is in contrast to, say, a large transformer where removing an important attention head post-hoc might not replicate the original explanation effect. Another test is to see if the system can answer “because” questions: e.g. if Manny answers a query with a certain path, can it use that path to correctly answer follow-up queries about the relationship? (This would show the path isn’t just plausible-looking, but contains accessible knowledge for deduction.)

Existing interpretability research provides tools for evaluation. Techniques like concept activation vectors and causal scrubbing examine if certain latent features correspond to human-meaningful concepts and if ablating them changes outputs. For Manny, the analogous approach would be to check if the high-curvature edges in a path correspond to known relevant relationships, and if zeroing those curvatures out causes the answer to change (indicating they were necessary). We anticipate that Manny’s explanations will be high-precision (when it shows a path, it’s usually relevant), but possibly lower recall (it might not show all factors a human would consider). Achieving ≥80% alignment with human judgments means in practice: for a set of test queries where ground-truth rationale is known (or at least agreed upon by experts), Manny’s highlighted path should match that rationale most of the time. Notably, models like KGReasoning for drug discovery have demonstrated such alignment – a system that finds drug–target interactions via known metabolic pathways was able to produce paths that experts judged as valid explanations in the majority of cases ￼ ￼ (Evidence A/B). This gives confidence that Manny’s path outputs can be similarly interpretable if the underlying knowledge graph is of high quality.

In terms of evidence grade: we have strong evidence (A) from knowledge graph literature that graph traversal with learned edge weights can produce human-comprehensible explanations. Manny is built on that principle. The risk is whether Manny’s continuously learned graph remains meaningful or becomes a tangle of adjusted edges that no human can follow. If, for example, Manny creates a shortcut edge with high curvature that doesn’t correspond to a real semantic relation (a “speed hack” that tricks the distance metric), then its explanations could become degenerate or misleading (e.g. “Why did you recommend this recipe? – Because apple→cinnamon edge had high curvature,” which might not make sense to a user if that edge was an artifact of training). We must monitor for such spurious edges. Mitigations include enforcing that edges are tied to some source of truth (e.g. only allow edges that were at least mentioned in the ingested knowledge, not completely random co-occurrences) and using the LLM as a sanity filter (e.g. prompt the LLM to describe the relation – if it can’t, the edge might be nonsensical).

Conclusion for H2: It’s likely that Manny can deliver in-principle explainability – each decision comes with a trace. The remaining challenge is to ensure these traces reflect valid reasoning. Empirically, we will grade this by human evaluation: given Manny’s “why” output, does a domain expert agree it answers why? We aim for ≥80% agreement to call H2 confirmed. If Manny’s explanations turn out to be uninformative (e.g. very long chains, or edges labeled with opaque IDs), that would falsify the spirit of H2. Early MVP tests using simple domains (like a recipe graph or a small academic subject) with known relationships will help calibrate this. Encouragingly, methods like GNNExplainer and Decision Set mining show that even learned graphs can yield concise subgraphs as explanations ￼. Manny’s design pushes this further by making the subgraph the computation. We will leverage that for clear visualizations (perhaps even interactive graph UIs in the explainer dashboard as a stretch goal).

H3: Reuse of Motifs for Transfer Learning

Hypothesis H3: Manny will autonomously discover and reuse motifs – frequent subgraph patterns corresponding to “skills” or knowledge chunks – leading to better transfer to new tasks. Concretely, if confronted with a new query or domain, Manny should repurpose ≥30% of relevant substructures from past interactions, improving sample efficiency (learning speed) compared to models that start from scratch or don’t explicitly mine motifs. In other words, knowledge gained earlier should help later tasks via overlap in graph components.

The concept of motif reuse parallels the idea of skills or option reuse in reinforcement learning and module reuse in neural networks – both areas where strong evidence shows efficiency gains. In hierarchical RL, for instance, having a library of sub-policies (options) that can be invoked for new goals often drastically improves sample efficiency. Huang et al. (2025) demonstrate that a hierarchical agent that reuses subgoal policies across similar navigation tasks learns faster and safer than a flat RL agent; by recycling learned behaviors for recurring scenarios, their HRL system converged to optimal policies with far fewer trials ￼ (Evidence B). Analogously, if Manny has, say, a “baking motif” subgraph from learning about apple tart, it could shorten the learning curve when asked about pear tart or cherry pie by reusing much of that subgraph. The Manny MVP includes a /save motif command and an automated motif mining during consolidation – we’ll measure how often these motifs appear in later solutions.

From the continual graph learning literature, there is evidence that preserving and reusing important subgraphs improves performance over naive approaches. Zhou et al. (2023) introduced a knowledge-aware replay in a streaming graph (Ethereum fraud detection) where they retain “inter-stage” edges (connections between new and old nodes) to transfer knowledge. Removing those connecting edges caused a steady performance decline, whereas keeping them enabled incremental performance improvements over time ￼ ￼ (Evidence B). In fact, their method matched the accuracy of retraining from scratch on each phase, while using far less data – a clear sign of successful knowledge transfer ￼. This aligns with Manny’s plan to keep high-valence edges (motifs) rather than pruning them, so they can be leveraged in future reasoning threads. Essentially, those motifs serve as “shortcuts” in the manifold for tasks that share partial structure.

Another relevant line of evidence is modular and compositional learning in neural nets. Research on Neural Module Networks and prompt-based compositionality shows that models can generalize to new combinations of learned components (like solving new tasks by combining known functions). Manny’s explicit motifs aim to capture such reusable pieces. For example, if Manny learns a subgraph for “make fruit tart” (connecting fruit–sugar–bake–tart), then faced with a new fruit, it ideally just plugs it into this motif rather than learning from scratch. The success criteria of ≥30% motif reuse can be checked by logging how many edges in new task paths carry a motif ID from prior tasks ￼. If Manny is truly reusing motifs, we expect a significant fraction of the edges or nodes in a new solution path were part of a saved motif (and likely with reduced traversal cost due to previous strengthening).

Evidence grade: Moderate (B) – The intuitions are backed by analogous findings in RL and graph continual learning, though direct “motif mining” systems are rare. We have concrete evidence of skill transfer in HRL (sub-policies reuse) and graph transfer (preserved edges aiding new classification tasks) ￼ ￼. On the other hand, classic neural nets without explicit modularity often struggle with negative transfer or forgetting, which is why this hypothesis is non-trivial. Manny’s contribution would be to explicitly identify and preserve potentially useful substructures. There is some precedent: unsupervised discovery of options in RL (e.g. via graph communities or reward-based clustering) has shown agents can accelerate learning new goals by composing existing options ￼ ￼ (conceptual evidence). Likewise, in procedural generation, having common sub-patterns (motifs) can speed up adaptation to new patterns.

What could falsify H3? If motif mining yields mostly trivial or very task-specific subgraphs that do not get invoked on new tasks, then Manny isn’t actually achieving reuse. For example, if Manny saves 10 motifs but when a new query comes, it either doesn’t match any motif or performs no better than if motifs were disabled, H3 fails. A negative outcome could be that Manny’s learned graph is so dense or entangled that motifs aren’t distinct or robust enough to apply to new problems (i.e. Manny ends up just memorizing each scenario separately, which some CL approaches devolve into). We should quantitatively compare Manny to a baseline like “no motif reuse” (imagine we turn off the motif cache and require it to learn each query fresh) – if performance (accuracy or steps to learn) is similar, then motifs didn’t help. Conversely, a strong positive signal would be something like: Manny answers a series of related queries faster and faster, or with fewer LLM calls, because it’s leveraging the subgraph built from earlier ones. The target 30% reuse means if a new query’s solution path has 10 edges, at least ~3 of them were seen before in earlier contexts (or come directly from a saved motif). This is an arbitrary threshold, but it sets expectation that significant chunks are reused. We should also monitor if reuse is positive (improves accuracy/efficiency) – reuse that causes negative transfer (using the wrong motif in a slightly different situation) would be a failure mode.

In the MVP, we plan simple tests: e.g., teach Manny about making an apple tart (motif saved), then ask about a pear tart – we expect it to reuse much of the “tart” motif (ingredients and steps) and thus require fewer steps or hints to get it right. The README acceptance test already describes this: after learning “apple tart,” a query about “pear tart” should produce a shorter path and motif reuse count increment ￼. We will replicate that and measure gains. Another scenario: sequentially learning related concepts (e.g. navigating a city map, then a nearby city – motifs could be common routes). If Manny can use >30% previous routes, that supports H3.

Bottom line: There is credible evidence that storing and reusing substructures can yield transfer learning benefits (see HRL and graph CL results above). Manny aims to operationalize this through motif mining. The MVP will confirm if those benefits materialize; if they do (with clear metrics on path lengths or sample efficiency improvement), it would be a differentiating strength of Manny over monolithic learners.

H4: Efficiency – Sparse, Local Updates on Event-Driven Hardware

Hypothesis H4: Because Manny updates only local parts of the graph per interaction and avoids wholesale backpropagation, it should achieve lower latency and energy use (≥30% reduction) compared to dense-learning models (like retraining a neural net for each new data) when deployed on appropriate hardware. In particular, the design maps naturally onto event-driven neuromorphic systems (spiking hardware) or analog memory arrays, which excel at sparse and local operations. This hypothesis has two parts: (a) computational efficiency on today’s CPUs/GPUs (by doing less work per update), and (b) even greater efficiency on specialized hardware (by leveraging sparsity and asynchrony).

Current evidence strongly supports the potential for massive energy and speed gains using event-driven neuromorphic hardware for sparse workloads. Intel’s Loihi 2 spiking neuromorphic chip, for example, has demonstrated over three orders of magnitude (1000×) better energy efficiency on certain optimization and search tasks compared to conventional CPUs ￼ (Evidence A). This was achieved by using sparse spike-based computation instead of dense matrix ops. Manny’s workflow – activating only a small subset of nodes/edges per query – is exactly the kind of workload neuromorphic chips can accelerate. In neuromorphic designs, energy is consumed only when events (spikes) occur, not continuously as in clocked systems. Thus an idle or sparsely active model draws almost no power. This leads to extreme energy-delay advantages for workloads like sensor streams or knowledge graphs that don’t fire all neurons at once ￼ (Evidence A). Manny’s local updates fit this regime: only the nodes along the path “spike” (get activated) and only those edges get weight updates (synaptic changes).

In fact, a Loihi-based demonstration exists for a graph search algorithm: by encoding graph edges as synapses with weights/delays, researchers used spiking neuron waves to compute shortest paths – the first spike to reach the target effectively found the shortest route ￼ (Evidence B). They leveraged local weight plasticity to mark visited edges, solving a classical graph problem in a brain-like fashion. This is a strong hint that Manny’s core operations (finding near-shortest paths and updating weights locally) can be implemented on neuromorphic hardware with similar or greater efficiency gains. Additionally, memristor-based analog compute has been used to solve graph problems in a massively parallel way: one study built a fully memristive spiking network to perform shortest-path graph learning, achieving high parallelism and energy efficiency by effectively doing analog breadth-first search over the graph ￼ (Evidence A). These examples underscore that Manny’s algorithmic parts have analogues that run orders of magnitude faster/cheaper on non-von-Neumann architectures.

In terms of current hardware (CPU/GPU), Manny’s efficiency will depend on how large the graph gets and how the ANN search (for nearest neighbors) is handled. In the MVP, for up to say 50k nodes, a brute-force or HNSW index for nearest neighbors is fine ￼. Each query would involve a few vector lookups and a graph traversal that is hopefully bounded (the thread length should be reasonable, not exploring the whole graph). Compared to an LLM-based agent that might generate many tokens or do expensive beam search, Manny could be lighter. For example, if a standard LLM agent uses 1000 tokens to reason a solution, but Manny can find a path and only use a 100-token call to the LLM for final answer formulation, that’s a 90% reduction in token (and thus compute) usage. H4 specifically targets a ≥30% wall-clock or energy reduction for equivalent accuracy. This is plausible: by doing continual learning, Manny avoids re-computation from scratch of past knowledge. Traditional neural nets, if retrained frequently, are very compute-intensive. Manny does tiny adjustments instead. Over many updates, that should integrate to big savings. As evidence, consider a scenario like online sensor fusion: an Intel neuromorphic study showed Loihi 2 performing sensor fusion with >100× energy efficiency vs a CPU, maintaining equal accuracy in real-time ￼ (Evidence A). The event-driven nature was key – only processing changes. Manny similarly processes each new query as a small change in the graph state.

However, it’s important to note potential overhead: Manny’s design uses an LLM as a semantic encoder/labeler (for embeddings, etc.). If the LLM is large and called frequently, that could dominate compute. The goal though (per H5) is to reduce reliance on the LLM to mostly labeling or periodic suggestions, not planning each step. If we achieve H5 (50% fewer LLM tokens vs an agent baseline), then the compute savings from fewer LLM calls plus sparse graph updates should indeed give >30% overall efficiency gain. We will quantify this in the MVP by tracking CPU time per interaction and, if possible, actual energy (with tools like Intel Power Gadget or estimating FLOPs). Efficiency will also be hardware-dependent: on a standard GPU, extremely sparse operations are not as efficient (GPUs like dense matmuls). But Manny could be run on CPUs or FPGA where sparse graphs are fine. In 12–24 months, we expect better availability of neuromorphic boards (Intel Loihi 2 should be more accessible to researchers; IBM’s digital synapse chips or even analog optical neural network accelerators might mature). If Manny proves the concept in software, porting it to such hardware could yield dramatic efficiency gains as seen in research prototypes.

Evidence summary: Very strong (A) for the qualitative claim that event-driven local learning is more efficient – numerous studies show 100×–1000× energy gains ￼ ￼. For the quantitative target (≥30% in our comparisons), we will have to measure on our specific tasks. We should be careful to compare fairly: e.g., Manny vs. a transformer fine-tuned online, or Manny vs. an LLM+retrieval that doesn’t update at all (the latter might have lower compute if it doesn’t learn; but then accuracy might drop as knowledge grows stale). Our bet is that as tasks scale, Manny’s continuous learning will pay off by not having to prompt huge contexts or retrain models from scratch, thereby saving time/energy. We will also simulate event-driven execution in the MVP (maybe count “spikes” = number of node activations) to extrapolate potential neuromorphic performance.

Finally, H4 includes scalability to interference-based hardware (like optical computing). Manny’s operations (dot products for similarity, and accumulation of curvature on edges) could be mapped to analog optical matrix operations (for fast similarity search) and optical interference patterns (for path finding). While speculative, there’s interesting research on photonic accelerators that perform associative memory or graph isomorphism quickly using light interference (some quantum-inspired algorithms treat path finding as a wave interference problem). These are beyond MVP scope but show that Manny’s graph math might align with those paradigms better than giant transformer models do.

In conclusion, H4 is quite likely to be confirmed if Manny works as intended. We will demonstrate at least on a small scale that per-update work is low and doesn’t grow with data (aside from graph size linearly). The MVP success criteria is a 15% latency reduction on a streaming task vs. a dense baseline (per brief), but we aim for more with fewer LLM calls and use of ANN indexes. Given the evidence, we consider H4 low risk: efficiency is one of Manny’s clear advantages on paper. The main caution is not to squander it by calling the LLM too often or by having to do heavy consolidation too frequently (if consolidation is too costly, it could eat into the gains – see Risk R3).

H5: LLM-as-Lens – Language Model as a Guide, Not a Controller

Hypothesis H5: Manny can maintain strong task performance while cutting down LLM usage by ≥50% (in tokens or calls) compared to an agent that relies on an LLM for planning/decision every turn. The large language model in Manny’s loop is used as a “lens” or suggester: it provides semantic mappings (embeddings, names, maybe heuristic suggestions), but it does not dictate the actions. The hypothesis is that this limited usage of the LLM is sufficient to achieve results comparable to full LLM-centric systems, thanks to Manny’s learned manifold doing the heavy lifting for reasoning and memory.

The motivation for H5 is multifold: cost (LLM API calls are expensive), efficiency (LLMs introduce latency), and alignment/control (we might want the LLM to not autonomously act, to reduce the risk of going off-script). Is there evidence that an LLM can successfully play a non-agentive, supporting role? Yes, in emerging LLM+knowledge integration research we see patterns: use the LLM to curate or extract knowledge, then use that knowledge in a separate mechanism for decision-making. For instance, Yang et al. (2024) propose Structure-oriented Retrieval-Augmented Generation, where an LLM first helps build a structured knowledge graph from unstructured data, and later queries retrieve from that KG to assist generation ￼ (Evidence B). In that paradigm, the LLM isn’t reasoning end-to-end each time; it’s providing a lens on raw data (turning it into a graph) and then the graph is used for actual QA or reasoning. Manny is analogous: the LLM might be used to embed text into the manifold or to label discovered motifs in plain language, but once the knowledge is embedded, Manny’s graph algorithms handle the rest (searching for answers, updating weights).

We also have evidence that relying purely on LLM context for memory has downsides, which justifies Manny’s more hybrid approach. LLMs suffer from context window limits and “context rot” – as a conversation or prompt grows, the model’s performance can degrade and it may focus improperly ￼ (Evidence C). Simply stuffing more facts into the prompt is inefficient. Techniques like retrieval (RAG) alleviate this by fetching only relevant snippets for the LLM. Manny’s graph can be seen as a sophisticated retrieval mechanism that brings only pertinent nodes into focus. Additionally, research indicates that LLMs alone do not learn from interactions in a lasting way (they have to be fine-tuned to truly internalize new info; otherwise each session is like Groundhog Day). By contrast, Manny’s graph does internalize updates. Jessy Lin notes that while in-context learning and retrieval can supply information, they don’t improve the model’s inherent capabilities – “we want the model itself to get better… not just retrieve previous sessions” ￼ (Evidence C). Manny achieves that by updating the manifold’s curvatures (a form of long-term memory) while keeping the LLM fixed. So H5 essentially hypothesizes that this separation of roles works: the LLM provides knowledge when needed, but Manny’s memory stores it for future use, reducing repeated queries to the LLM.

Empirically, we will measure how often the LLM is consulted. A baseline to compare could be an LLM-based agent (like AutoGPT or ReAct) solving the same tasks. Those typically query the LLM at every reasoning step and possibly with lots of tokens (chain-of-thought, tool use descriptions, etc.). If Manny can answer many follow-up questions by traversing its graph without calling the LLM’s reasoning each time, that’s a big win. For example, imagine a user teaches Manny a concept via an LLM call (ingesting some text), then asks various questions. Manny might be able to answer some just by graph lookup (no new LLM call) or with only a final phrasing call. This could easily halve token usage. We aim for ≥50% reduction as a concrete target. This is plausible because Manny will remember intermediate results (the graph is a form of scratchpad), whereas an LLM agent often repeats reasoning from scratch each time or has to be re-prompted with context.

One piece of supporting evidence is from the field of conversational agents with memory. A recent work “Generative Agents” (Park et al. 2023) used an LLM to simulate characters with long-term memory by storing notes in a database and retrieving relevant notes for the LLM to condition on. The LLM’s role was still central (it generated behavior), but the memory significantly cut down the needed prompt size and allowed consistency. Manny can be seen as an automated version of such memory: the curvature and motif structure bring up relevant notes. If successful, Manny would demonstrate that a smaller LLM (or fewer calls to a big LLM) is enough if the structured memory is good.

We also consider alignment benefits: using an LLM as a constrained tool (embedding text, summarizing a node, etc.) means we avoid giving it autonomous agent control, reducing risk of it producing harmful or off-task actions. Manny can thus be more predictable and easier to guardrail – the LLM outputs can be filtered or verified when they are only used for labeling, and Manny’s decisions are ultimately governed by its graph heuristics. This partitioning is supported by the idea of “LLM in the loop” systems where the LLM is overseen by a deterministic process. Early experiments (e.g. an LLM that suggests code which is then executed with tests) show that the combination can outperform the LLM alone in reliability. Manny similarly would consult the LLM for insight (like “give embedding for this new concept” or “suggest a link between these two nodes”), but then integrate that in a controlled way.

Evidence grade: We assign this B/C – it’s a newer idea, but grounded in the trends of reducing reliance on prompt length and using structured knowledge with LLMs ￼ ￼. The success of H5 will be determined by experiment: if Manny’s accuracy on tasks stays high while calls to the LLM are infrequent, we’ve proven the hypothesis. If, on the other hand, we find that Manny constantly needs the LLM to figure out what to do next (e.g. the graph search fails without the LLM guiding it), then H5 fails – Manny would have collapsed back into an LLM-centric agent. One way to test this is to run Manny in a mode with no LLM after initial learning and see if it can handle a sequence of prompts on its own. Another test is budget enforcement: limit LLM tokens and see if Manny still performs; a robust Manny should degrade gracefully, solving simpler queries from memory and only going to the LLM for genuinely new knowledge.

In the MVP, we will track the token count used by Manny versus a baseline agent. For instance, if a user asks 10 questions, an agent might generate a few hundred tokens of reasoning each time (say 5000 total), whereas Manny might use a 100-token answer generation for each (1000 total) after having built its graph – that would be an 80% reduction. We’ll also measure any quality difference (e.g. using automated metrics or human rating for correctness). If quality stays within say 5-10% but tokens drop by half or more, H5 is confirmed. We already have Manny’s CLI supporting some of this: commands like /learn ingest=domain use the LLM once to parse data, then multiple queries can be answered from the graph with a simple template.

Risks: One potential issue is if the LLM is actually needed more than anticipated. For example, Manny might struggle to resolve ambiguity or do abstract reasoning the graph can’t capture, forcing LLM usage. We should carefully delineate what tasks Manny can do alone (e.g. factual recall, shortest path reasoning, simple analogies if embedded) versus what needs an LLM (open-ended commonsense, generating fluent language answers). Our design keeps the LLM for that final step – which is fine, since surface realization of answers is what LLMs are great at. The key is they’re not used for the internal reasoning path.

In conclusion, if Manny achieves H5, it means we’ve created a more cost-effective and controllable system by leveraging the LLM’s knowledge only when necessary. Given the evidence of combined systems and the known drawbacks of over-relying on prompts ￼ ￼, we are optimistic that H5 is attainable. The MVP will explicitly log LLM usage to quantify this improvement.

Prior Art & Differentiators

Manny Manifolds touches on multiple research fronts. Below we map key comparators in each area, summarize their status, and highlight how Manny differs:
	•	Continual / Online Learning: Traditional approaches include regularization-based methods like Elastic Weight Consolidation (EWC) ￼, Synaptic Intelligence, and Memory Aware Synapses, which slow down changes to important weights to avoid forgetting. There are also rehearsal methods (experience replay buffers of past data) and dynamic architecture methods (growing networks or gating to isolate new task parameters). These methods have had success on benchmark tasks (e.g. EWC on permuted MNIST prevented forgetting to a large extent ￼). However, most assume clear task boundaries and still struggle with long-term integration (they mitigate forgetting but rarely improve past knowledge). Manny differs in that it does online, task-free learning – every user interaction causes a tiny update, with no explicit task segmentation. Instead of global weight regularization, Manny localizes updates to the specific concept relationships used. It’s closer to Hebbian lifelong learning or neuromorphic approaches, whereas mainstream CL is still largely focused on batched tasks. Manny’s periodic consolidation (the /sleep command) has some analogy to replay (it prunes low-weight edges and mines motifs akin to re-training on important bits), but it avoids storing raw past data, operating within the knowledge graph domain. In summary, Manny is more granular and graph-based than typical CL neural nets. Its success would demonstrate an alternative path for continual learning: embedding knowledge in a graph manifold with ongoing plasticity rather than only in neural network weights updated episodically.
	•	Graph Neural Networks & Geometric ML: Graph Neural Networks (GNNs) like GCN, GAT, GraphSAGE have achieved great performance in static graph tasks (node classification, link prediction) by aggregating neighbor information. Extensions like Dynamic GNNs handle evolving graphs in discrete time steps, often via re-training or incremental embedding updates ￼. Manny shares with GNNs the idea of representing entities and their relations explicitly. But a big difference is Manny doesn’t do backprop gradient training on the graph; it uses an interactive traversal and local update paradigm. In spirit, Manny is closer to energy-based models on graphs or path-finding algorithms than to message-passing neural nets. For example, Manny’s thread running is reminiscent of a random walk or heuristic search on a weighted graph, rather than computing an embedding for each node via layers. One can liken Manny to a personalized PageRank that keeps adapting link weights with usage. Another related area is Topological Data Analysis (TDA): using curvature or topology of a data manifold to understand structure. Manny’s notion of “curvature” on edges as a function of use/valence is novel, but conceptually it’s like introducing a Ricci curvature or edge tension that changes with experience – there have been works in network science using curvature (e.g. Forman-Ricci curvature) to analyze graph structure, but not to learn in real-time. Manny could be seen as implementing a form of Ricci flow on a knowledge graph (a playful analogy: the graph’s geometry is reshaped gradually to optimize some objective like shorter paths for important queries). No standard GNN does that. Energy-based and predictive coding models (e.g. Hopfield networks, PCNs) are perhaps the closest analogues: they also adjust weights to settle at minima for presented patterns. One can argue Manny is an energy-based memory model where curvature plays the role of energy landscape – high-curvature edges are low-cost paths (attractors). Unlike typical EBMs, Manny is explicitly exposed to the user (you can query “/why” to see the energy minima path), which is a differentiator in terms of transparency.
	•	Memory & Skill Learning: Many AI systems use external memory: from classic key–value memory networks to today’s vector databases with retrieval (RAG). Those systems append or index memories but don’t change the model’s internal parameters with each addition (so no long-term consolidation, and potential inconsistency between memory and model). Manny’s graph is the memory and the model simultaneously (more like a knowledge base than an external DB). It performs a kind of experience replay internally by strengthening frequently used edges (like replaying those associations) and by motif extraction (like distilling frequent patterns, analogous to “option discovery” in RL). In reinforcement learning, the idea of options or skills allows an agent to call a multi-step policy as a single action. Manny’s motifs are conceptually similar – a motif could be traversed as a unit, effectively compressing multiple hops into one reusable chunk. Some prior systems in hierarchical RL (e.g. FeUdal Networks, Option-Critic) required a lot of reward engineering to discover useful options; Manny instead will use frequency/valence as a heuristic to identify motifs, which is simpler but might miss less obvious yet useful skills. Another comparator is program induction or neural program synthesis, where an agent learns subprograms to reuse (e.g. DreamCoder, which finds functions from examples). Manny doesn’t attempt to induce code, but motifs are like subroutines found by looking at the graph’s frequent subpaths. Manny’s uniqueness is treating the data itself as a space (manifold) and conversation as movement – whereas most memory systems treat memory as a set of snapshots or keys to retrieve. By having a spatial metaphor, Manny can reason in terms of distances, neighborhoods, and detours, which might offer novel ways to generalize (e.g. analogy as finding a similar curve in the manifold).
	•	Interpretability & Mechanistic Insight: We’ve discussed how Manny provides explicit paths as explanations (H2). The comparators here include mechanistic interpretability of transformers – researchers try to decipher the circuits or neuron roles inside trained transformers (finding linear representations of concepts, tracing forward passes, etc.). Those efforts have yielded some understanding (like attention heads that correspond to certain syntactic relations, or neuron clusters that track specific facts), but it’s extremely resource-intensive and still incomplete. Manny’s approach could bypass the need for post-hoc probing because the “circuits” (paths) are first-class objects in the model. Another comparator is causal graph-based explainers like Causal Scrubbing (which identifies which parts of a model’s computation graph are causally relevant to an output). Manny essentially exposes its computation graph to the user directly; one could perform causal interventions on Manny (e.g. knock out an edge) and see effect on output easily, which is much harder in a dense neural net. For explaining graph-based ML, there are methods like GNNExplainer (which finds a small subgraph that most influences a GNN’s prediction) ￼. Manny makes that intrinsic: it always finds a subgraph (path) for each answer. The difference is Manny’s answers are actually generated via that path, whereas GNNExplainer’s subgraph is an ex post explanation for a forward pass. This gives Manny a potential edge in trustworthiness – the explanation is directly tied to how the answer was computed, reducing chances of deception. On the flip side, a risk differentiator is that Manny’s answers are constrained by needing a path in the graph; if the graph lacks a needed connection, Manny might fail to answer even if an LLM could hallucinate a plausible answer. In other words, interpretability might trade off raw flexibility. This is a known trade-off in AI safety: more constrained, interpretable models can be less immediately powerful than black-box LLMs. The hope is Manny’s continual learning mitigates that by gradually filling in its graph.
	•	Hardware (Neuromorphic, Photonic, Quantum-inspired): Traditional deep learning is mapped to GPUs/TPUs with dense linear algebra. Manny’s design is more suited to neuromorphic hardware like Intel Loihi, IBM TrueNorth, SpiNNaker, etc., which implement neuron-like and synapse-like primitives. Those chips excel at sparse, event-driven computation and can do on-chip local learning like STDP (spike-timing dependent plasticity). Loihi, for example, supports user-defined plasticity rules that could implement Manny’s curvature update rule (triggered by traversal events with a valence signal) ￼ ￼. If we were to implement Manny on Loihi, we’d treat nodes as neurons, edges as synapses with weights = curvature, and a thread traversal as a spike packet from start to goal neuron. The local delta-k updates are analogous to reward-modulated STDP, which Loihi can do with its programmable microcode for synapses ￼. No standard ML system aside from SNNs could run so naturally on that hardware. This is a differentiator: if neuromorphic computing advances (and Intel reports 5000× faster-than-real-time spiking speeds in Loihi 2 for some tasks ￼), Manny could ride that wave, whereas Transformers would need conversion to spikes (an active research area but not straightforward). For photonic computing, some startups and researchers are building optical neural networks or analog Hopfield networks. Manny’s graph operations (matrix-free, pointer-heavy) might not directly map to photonic matrix multiplication, but interestingly, light propagation through analog circuits has been used to solve graph problems by treating path length as phase and interference as combining paths. Manny’s continuous manifold idea conceptually fits analog computing: you could imagine encoding edge weights in analog resistances or optical attenuators, and letting a wave front find the least “resistance” path. In fact, a fully memristor crossbar-based SNN was built for shortest path as mentioned ￼ – that’s essentially analog hardware finding paths. Manny could leverage similar designs (e.g. using memristor arrays to do breadth-first search in parallel). While this is forward-looking, it means Manny might scale with emerging hardware better than enormous transformers which strain digital hardware budgets. Quantum-inspired algorithms (like quantum walks on graphs) could also potentially accelerate path finding – Manny’s problem (shortest/cheapest path in a dynamically weighted graph) is something quantum computing might address (though current quantum approaches are nascent for this domain).

Summary of Differentiators: Manny sets itself apart by combining features rarely seen together: fine-grained continual learning (from CL research) with a graph-based knowledge representation (from KGs/GNNs), explicit skill/path reuse (from RL options) with first-class interpretability (from XAI methods), and a design that’s neuromorphic-friendly. Each of these domains has mature prior art, but their combination is novel. The closest prior systems might be “cognitive architectures” or “knowledge graphs + reasoning engines” (like Cyc or OpenCog), which also maintain a symbolic graph of knowledge and update it. Manny differs by being interaction-driven and self-organizing (curvature updates via usage) rather than requiring manual knowledge engineering. One might also compare Manny to Personal Assistants with knowledge graphs (e.g. Apple’s Knowledge Navigator concept) – Manny could be seen as learning a personalized KG for the user, with the LLM helping to populate it. The key new hypothesis Manny brings is the curvature mechanism for learning and using the knowledge graph. If successful, Manny will have shown a viable path alternative to end-to-end neural behemoths: an approach where data becomes the model, continuously shaped by use, and still interoperating with powerful pre-trained models (LLMs) in a controlled way.

Risk Register (Key Risks & Mitigations)

Below we enumerate major project risks identified (R1–R5), their potential impact, and mitigation strategies. Each risk is rated for Severity (considering likelihood and consequence), and we outline how to detect or manage each:
	•	R1: Plasticity Tuning is Brittle – Manny’s learning rate or update rule might be too sensitive, causing either divergence (if too high/plastic) or stagnation (if too low). Severity: High. If we can’t tune η (eta) and related parameters to maintain stability across scenarios, Manny fails at the foundational H1. We might see oscillating curvatures or total freeze. Mitigations: Start with very conservative learning rates (small η) and use the stability monitors (variance of curvature, number of edges growing) to auto-adjust. Implement an auto-throttling: if curvature variance spikes or many edges saturate at clamps, automatically reduce η or increase decay. Conversely, if learning progress is too slow, allow a slight η increase. Meta-learning could be applied: use a small set of training tasks to optimize the plasticity hyperparameters for minimal forgetting and good adaptation (similar to learning rate schedules in LSTM meta-learning). We will also constrain updates per turn (the TURN_KAPPA_BUDGET already does this ￼) to cap any single interaction’s impact. In short, treat Manny’s plasticity like one would treat a delicate optimizer – with scheduling, gradient clipping (we have curvature clipping), and careful initialization (e.g. start with zero curvature and gradually allow growth). We’ll test extreme cases (rapid-fire conflicting inputs) to see if it remains stable, and adjust the rules accordingly (perhaps introducing a normalized update rule or using relative changes).
	•	R2: Interpretability Becomes Decorative – There is a danger that the “reasoning paths” Manny shows are not truly the causal factors for answers, but just a byproduct or even misleading. This could happen if Manny relies on the LLM hiddenly or if shortcuts form in the graph that aren’t meaningful to humans. Severity: Medium. Even if Manny works, if the explanations are not trustworthy, one of its selling points evaporates (and it could mislead users or engineers). Mitigations: We will pre-register interpretability tests: for any evaluation question, define what a valid explanation should look like (like a ground truth chain of reasoning or a set of known relevant concepts). Manny’s output can then be checked for whether it hits those points. Also implement predictive explanation checks – e.g., hold out a key edge and see if Manny’s answer changes (if Manny says ingredient X is why a recipe works, remove X’s edge and the answer should degrade). If explanations are consistently spurious, consider adjusting Manny to incorporate causal attribution: e.g., weight edges not just by static curvature but also compute an ablation importance by temporarily dropping them during reasoning to see impact. We can incorporate that into the /why command (highlight in red any edge that if removed would alter the answer). This ensures the shown path is indeed critical, not just decorative. Another mitigation is to avoid over-relying on a single path if multiple exist – Manny could report multiple top paths or a confidence for the path. If the top path is a fluke, usually another will take over if the answer truly rests on different info. In user interface, we’ll present explanations with uncertainty: e.g., “Path (confidence 0.9) that led to answer…” so users know how much to trust it. And as a guardrail, if Manny’s explanation routine detects something odd (like a very high curvature edge that was never validated by any ingest or user feedback), we might flag or filter it. In summary, we will validate Manny’s explanations against known ground truth and intervene if they diverge. This keeps interpretability genuinely useful, not just a marketing gloss.
	•	R3: Consolidation Cost Dominates – The offline “sleep” phase (rebuilding indexes, mining motifs, pruning edges) could become a bottleneck, negating the efficiency gains or even causing downtime. If Manny’s graph grows large, consolidation might be expensive, or if done too frequently it could constantly interrupt online operation. Severity: Medium. A long pause to consolidate or heavy compute usage would hurt user experience and undermine H4. Mitigations: Use a budgeted or amortized consolidation strategy. For example, limit consolidation to at most N seconds per M interactions, or do it incrementally (a few edges pruned each interaction rather than all at once). We can keep a running window (“daily diff”) as Manny already tracks via timestamps ￼, and only consolidate the region that was active recently – this way, if most of the graph is static, we don’t keep reprocessing it. For motif mining, implement a streaming top-K motif search rather than exhaustive: maintain counts of traversed subpaths in an online manner (perhaps using a trie or frequent pattern mining algorithm that updates incrementally). This way, when sleep is called, it may already know the top motifs and just finalize them. We’ll also profile the ANN (approximate nearest neighbor) rebuild – in MVP, HNSW index rebuild for 50k nodes is okay, but if it wasn’t, we could switch to a dynamic ANN structure (some libraries allow insertions without full rebuild). If consolidation still proves heavy, a trade-off is to do it less often; perhaps only when user explicitly triggers /sleep or at certain idle times. We will instrument how consolidation time scales with graph size to anticipate any problems. Essentially, optimize the consolidation algorithms and set sensible schedules (maybe nightly consolidation for a persistent Manny, etc.). Another angle: if Manny runs on hardware with many cores, put consolidation on a background thread or separate core so it doesn’t block answering. The risk is known, but manageable with engineering effort.
	•	R4: LLM Dependence Creeps Back In – Despite intentions, the LLM might end up being used more and more (for complex reasoning that the graph can’t handle, for parsing, for bridging gaps), effectively turning Manny into just an LLM wrapper. This would fail H5 and also reintroduce cost/safety issues. Severity: High (it undermines the whole point if Manny can’t function without constant LLM help). Mitigations: Enforce a strict interface budget for the LLM. For example, limit calls per query to 1 (or a small fixed number) and design around that. If Manny tries to ask the LLM repeatedly, that’s a sign the graph needs more information – so maybe the knowledge ingestion was incomplete. We can prompt the LLM to give structured and rich info in the one call (e.g. when ingesting a topic, have it output a list of key relations we can directly insert as edges, rather than having to query those relations one by one later). Also, penalize or log heavy LLM usage: have a counter, and if it spikes, treat it as a bug. To avoid subtle over-reliance, we will test Manny on scenarios entirely offline (no LLM) after initial setup – it should handle straightforward variations or follow-up questions without new LLM input. If it can’t, then it’s too LLM-dependent and we need to enhance the graph algorithms. Possibly, implement alternative non-LLM tools for some functions: e.g., use WordNet or a local embedding model for synonymy rather than calling GPT-4. Use a small language model fine-tuned for simple tasks (like formulaic responses) if needed, instead of a big one. Essentially, treat the LLM as a bootstrapping tool that we wean Manny off of over time. By the 90-day mark, aim to demonstrate Manny solving some problems with 0-shot LLM calls during the problem-solving (only using LLM for pre-processing or final answer phrasing). This will prove Manny’s autonomy. From a project management perspective, clearly define which functions are LLM-powered and try to minimize them. If new feature requests come up (“Can Manny handle creative queries?”), resist piping them straight to LLM – find a way to extend the graph’s capabilities or simply say that’s out of scope. Keeping this boundary will ensure Manny’s core value (a learned manifold) isn’t drowned out by the lure of just using the LLM for everything.
	•	R5: Hardware Lock-In or  Mismatch – There is a risk that Manny’s design might favor exotic hardware that isn’t readily available, or conversely, that it runs poorly on standard infrastructure. For example, if we highly optimize for spiking neuromorphic chips, but those aren’t production-ready, we could end up with a solution that is impractical. Or Manny might require large memory for the graph, which could be an issue on edge devices. Severity: Low in 90-day MVP (we can use normal hardware), but Medium longer-term if Manny is to be productized. Mitigations: Simulate and abstract the hardware operations first. We can implement Manny’s core as simple Python/Numpy to ensure it conceptually works, then profile the bottlenecks. If standard hardware is enough (which it likely is for moderate graph sizes, since 50k nodes and sparse edges is not huge for a modern PC), we’re fine. But we also outline a minimal operator set Manny needs (see Hardware Mapping section) – basically: vector similarity search, graph neighbor lookup, and weight update. All of these can be done on CPU, and accelerated if needed with GPU for vector math or with indices. In 12–24 months, if we want to deploy Manny on neuromorphic or analog, we should keep the design modular so that, say, the similarity search could be swapped out for a neuromorphic implementation and the thread traversal algorithm could run on Loihi. We’ll coordinate with hardware experts (if available, e.g., maybe allocate a specialist to try deploying on Intel’s Loihi cloud or so as a parallel track – this might be beyond 90 days but could be in a follow-on). Essentially, we avoid any commitment that Manny must have neuromorphic hardware; we ensure it runs decently on conventional setups, proving value first. If specialized hardware is delayed or not as beneficial as hoped, Manny should still be competitive algorithmically (with perhaps a slightly higher cost that’s still acceptable if it provides better interpretability or learning). Conversely, to avoid missing the hardware boat, we’ll keep an eye on neuromorphic availability (Intel plans dev kits for Loihi 2, etc.) and perhaps prepare a small prototype mapping (like simulate Manny’s learning rule as STDP and see if Loihi’s API can support it). Having a clear understanding of what a future Manny chip would require helps us not inadvertently use operations that are impossible on those platforms (for example, avoiding floating-point heavy global ops, which we do avoid). So the mitigation is flexibility: design Manny to be portable, and test on both a low-power device (like a Raspberry Pi, to simulate edge deployment) and a regular server to gauge performance and memory. This will reveal any glaring mismatch early (e.g. if Manny’s graph eats 16GB RAM with 100k nodes, that’s a problem – we might compress embeddings or quantize curvatures, which also incidentally makes it more hardware-friendly).

Each of these risks has an owner in the team: R1 and R3 (stability, consolidation) will be mainly on the ML engineer/researcher; R2 (interpretability) on the research lead in alignment with a UX person for how to present explanations; R4 (LLM usage) on the integration engineer or product manager to enforce budgets; R5 (hardware) on the engineering lead/architect to coordinate with any hardware providers. We will maintain a dashboard of these risk indicators (e.g., a chart of LLM tokens per query over time for R4, a chart of average curvature change for R1, etc.) as part of our dev process. This way, we catch issues early. The above mitigations will be refined as we gather data – e.g. if we see plasticity issues, we might consider dynamic per-edge learning rates (valence could modulate η per edge, effectively giving important edges slower change – akin to EWC but local). The risk register will be revisited at each milestone.

90-Day MVP Experiment Plan

To validate Manny’s feasibility, we propose a series of experiments and evaluations aligned with the core hypotheses (H1–H5) and success criteria. These will use a small number of datasets/streams and compare Manny to baseline approaches. All experiments will be implemented in a reproducible way (likely as Jupyter notebooks or scripts in the repo), with clear metrics and logging for analysis.

Datasets / Streams

We will evaluate Manny on two primary scenarios (plus a third stretch goal scenario if time permits) to cover both structured knowledge and sequential learning settings:
	1.	Text Reasoning Stream (QA in a Narrow Domain): A synthetic but illustrative text-based domain where new facts or rules arrive and queries must be answered from them. For example, a “Recipe Assistant” domain: initially, Manny ingests a few recipes (small text descriptions) for apple pie, chocolate cake, etc. Then the user asks questions like “How do I substitute sugar in apple pie?” or “Can I use pears instead of apples in that tart?” which require reasoning with the known recipes. Over time, new recipes or ingredients are introduced. This stream tests if Manny can learn continually (e.g. incorporate a new recipe) and answer questions by finding relevant ingredient/step connections. The domain is narrow so that we can define ground-truth reasoning paths (for evaluation of explanations). We will generate a tiny dataset (maybe 5–10 recipes and 20–30 questions) to simulate this evolving knowledge base. Alternatively, we could use a public dataset like bAbI tasks (Facebook’s 20 QA tasks) specifically the ones about supporting facts and lists, which require multi-hop reasoning. However, bAbI is static per task, so we might just take inspiration from it. The “recipe” domain is nice because it’s intuitive for humans to judge motif reuse (common steps) and explanation alignment.
	2.	Knowledge Graph Stream (Structured incremental facts): We will create or take a small knowledge graph that evolves. One idea is to use a subset of Wikidata or ConceptNet and simulate time or version updates. For example, start with a mini knowledge graph of world facts (capitals, leaders, etc.), then update facts (new president, etc.) or add new entities, and query the system. This tests Manny’s ability to update facts without forgetting and to use motifs (like a chain of relationships) across entities. A concrete example: knowledge graph of a fictional universe (like a tiny dataset of characters and relationships in Game of Thrones). Stream in new relations as “new episodes” come out. Ask questions like “Who is Jon Snow’s ally’s father?” that require multi-hop traversal. As new episodes arrive, relationships change or new characters appear; Manny should integrate these and still answer earlier questions correctly. We can evaluate accuracy versus known answers and see if Manny’s path answers match known relationship chains. This scenario is structured and can be easily represented as a graph from the start, so Manny’s input is basically feeding triples (possibly via the LLM to turn text into triples). It’s a good test for H1 (not forgetting old triples), H3 (reusing subgraphs like family trees or alliance networks), and H2 (explanations as graph paths are naturally checkable by humans). The size will be small (maybe 50–100 nodes and dozens of relations total). Alternatively, if we want real data: the ICEWS event database (daily country events) could be streamed, but that’s quite large and noisy for MVP. We’ll stick to a controllable synthetic KG.
	3.	(Stretch) Multimodal Sensorimotor Toy World: If time allows, we test Manny in a simple simulation (e.g. a grid-world or a path-planning puzzle). For instance, an agent navigating a maze where walls can appear/disappear (streaming changes in the environment). Manny’s nodes could represent locations or states, edges possible moves, and curvature might represent “difficulty” or frequency. As the agent experiences the world, Manny updates path costs. The queries could be “find a path to X” or “what’s the quickest way to the goal now?”. This tests energy/latency (since path finding is directly the task) and whether local updates help adapt to changes (like if a route becomes blocked, Manny should quickly adjust). We would compare to, say, A* algorithm re-running from scratch each time as a baseline. If Manny reuses its learned curvature (like it had a motif for going around obstacles), it might adapt faster. This scenario primarily targets H1 and H4. However, given time constraints, we may deprioritize this in favor of focusing on the first two (which already mix reasoning and learning).

Baselines

For each scenario, we’ll compare Manny against relevant baselines:
	•	LLM+Retrieval Baseline: This is a system that doesn’t learn at all, but uses the LLM with a knowledge retrieval component. For the text QA domain, the baseline would be: store all input texts, and for each query, do a semantic search (embedding-based) to fetch relevant text and ask the LLM to answer from that (RAG-style). It will use no learned adjustments between queries (no parameter updates). This tests Manny’s ability to improve with experience versus a static memory. We expect Manny to surpass this baseline in scenarios with repeated or related queries (because Manny will shorten paths or integrate info, whereas retrieval+LLM does the same every time). Metrics: accuracy of answers, number of LLM tokens used, and response time. The baseline might use GPT-4 fully to reason and answer, so likely high token usage and always correct (if the context is found). Manny should aim to match correctness but with fewer tokens and perhaps faster response after some learning.
	•	Finetune/Retrain Baseline: In the knowledge graph scenario, a baseline could be a Graph Neural Network retrained periodically. For example, use an R-GCN or DistMult embedding for the KG; whenever new facts come, retrain or fine-tune it slightly (with replay of old facts to avoid forgetting). This baseline addresses whether Manny’s continual updates are as good as just re-training a model on the growing data. We measure accuracy on queries and the computation time used for each update. Manny’s advantage should be speed (no full retraining) and potentially better retention if retraining is partial. Another baseline in CL is experience replay memory explicitly – but in a knowledge graph, replay is essentially just re-learning from all facts, so that’s similar to full retrain.
	•	Transformer Agent Baseline: We can simulate a simplified AutoGPT/ReAct: the agent uses an LLM to think step-by-step for each query, possibly with chain-of-thought prompting. It might use tools like a calculator or a wiki browser if needed (depending on domain), but importantly it doesn’t update any persistent memory between queries except what’s in the conversational context. This baseline will likely have correct reasoning for each query given enough prompting, but it’s effectively stateless learning-wise (aside from what’s carried in the conversation window, which is limited). We compare how many tokens it needs and if it repeats mistakes that Manny learns to avoid. For example, if a tricky question requires two reasoning steps, the agent will figure it out each time from scratch (or require the user to remind it), whereas Manny might solve it faster the second time due to stored edges.
	•	Ablation Baselines within Manny: We will run Manny in ablated configurations to isolate contributions: (a) No motif mining – Manny updates edges normally but doesn’t save or reuse composite motifs (to see if motifs gave an extra boost to transfer), (b) No curvature updates (static) – Manny essentially becomes a static semantic graph (edges weighted only by initial similarity or co-occurrence) to see the value of learning (this reduces Manny to a retrieval graph), (c) No consolidation/decay – let Manny run without ever pruning or decaying edges to test if it collapses or gets slower (stability test), (d) No valence weighting – treat all usage as equal (η * 1 * f(usage)), ignoring any user valence feedback, to see if the valence mechanism adds value (e.g. perhaps valence helps highlight relevant info and without it Manny might clutter the graph with trivial info equally). Each ablation is evaluated on the same metrics to check which components are critical for success. For instance, if “no motifs” significantly increases sample requirement for new tasks, we confirm H3’s importance.

Metrics & Success Criteria

We will quantitatively evaluate the following metrics, aligned with the criteria given:
	•	Convergence (Path Shortening): For repeated or related queries, measure the length (number of hops) of the solution path over time. Success is ≥20% reduction on average for the second time a similar query is asked vs. the first time ￼. For example, if answering “pear tart” originally took 5 hops (with detours) and after learning from “apple tart” it takes 3 hops via a saved motif, that’s a 40% shortening – good. We’ll log path lengths per query and look at trends.
	•	Transfer & Motif Reuse: When a new query arrives that could use past motifs, we check what fraction of its edges or subpath come from saved motifs. We also track motif reuse count (the system increments reuse count when a motif is used ￼). Success criterion: ≥30% of new query’s route is composed of previously seen subpaths (and ideally the model solves it with fewer steps or higher accuracy than a baseline without that experience). Also, measure sample-efficiency: how many queries or examples did Manny need to solve a new type of query, compared to baseline. If possible, compute an Area Under Learning Curve or similar. For instance, if baseline needs 5 demonstrations to get something and Manny needs 2 because it reused a motif, that’s a win. In tasks like the KG stream, if a motif “X is friend of Y is friend of Z” was mined, and later a question “X and Z relationship?” can be answered by reusing that motif, we quantify that.
	•	Accuracy / Quality of Answers: Though not explicitly listed, obviously Manny must answer questions correctly. We will measure accuracy or error rate on queries (where ground truth is known, like in the KG domain or synthetic QA). Manny should match baseline accuracy (within a small delta) to be viable. We also measure if accuracy drops over time (a sign of forgetting or drift, which we don’t want). Stability metric from the brief: edge growth <5% per epoch – in our context, maybe after each consolidation “cycle”, the number of edges shouldn’t blow up. We’ll watch number of nodes/edges: if Manny keeps adding nodes for every word despite consolidation, that’s a problem. We target a sparsity (maybe maintain >85% sparsity as coded ￼).
	•	Latency & Efficiency: Measure wall-clock time per query (or per batch of interactions) for Manny vs baselines. Also measure number of LLM tokens consumed. Success criteria: Manny uses ≥15% less time per interaction after learning, and overall ≥30% less compute or energy vs dense baseline for equivalent accuracy. For token usage, success is ≥50% reduction compared to an agent baseline on the same task (H5). We’ll instrument token counts easily if using OpenAI API (they return usage) or we can simulate for consistency. For energy, we might approximate via CPU utilization or simply note algorithmic complexity differences. If we have time, use a power meter for a long-running experiment to integrate actual joules – but simulation may suffice (like X model took Y seconds on CPU, Manny took Z seconds, assume power P, so energy PZ vs PY). We expect Manny’s algorithmic complexity per query to be roughly O(d * log N) (d = path length, N nodes for ANN search) versus a transformer which is O(n^2) self-attention on prompt length plus potential fine-tune cost.
	•	Stability (No Catastrophic Forgetting): We will design a “retrain test”: After a sequence of learning, test the model on some earlier queries or facts to see if it still answers correctly. Also, monitor curvature variance – we can log the distribution of curvature values after each update or epoch. Ideally, it stays bounded (e.g., mean curvature maybe shifts slowly, no wild oscillations). Another measure: backward transfer in CL metrics – measure performance on initial tasks after learning new tasks. If Manny’s backward transfer is near 0 or positive, that’s good (it hasn’t forgotten, maybe even improved some answers if new info clarified something). We expect Manny might even improve old answers if motifs connect them better. We’ll compare to a baseline that often suffers forgetting (like a sequential fine-tune without regularization would drop accuracy on earlier tasks significantly).
	•	Explainability (Human Alignment of “Why” Paths): We will perform a user study or heuristic evaluation on Manny’s explanations. For a sample of queries, we’ll have either ourselves or domain volunteers rate whether Manny’s /why path satisfactorily explains the answer. We aim for ≥80% of these to be rated as aligned or understandable (as per success criterion). Concretely, if we have 10 test questions with known reasoning, at least 8 of Manny’s shown paths should hit the key points. We can define a scoring rubric: +1 for each relevant node on the path, -1 for each irrelevant or extra node, etc., and see if Manny’s explanations score above a threshold. Additionally, we can measure if the explanation predicts the answer choice: e.g. in multiple-choice scenario, would a human picking an answer based on Manny’s path pick the same as Manny did? If yes 80% of time, that indicates alignment. This is a bit involved for MVP; at minimum, we’ll verify qualitatively that Manny’s /why makes sense in our small domains (like it mentions the correct recipe ingredients that led to the suggestion).
	•	Token Economy: As mentioned under efficiency, count LLM tokens. If Manny achieves the ≥50% reduction, great. If not, document why (maybe the LLM had to intervene for reasoning gaps).

We will present these metrics in a dashboard or report per experiment. For example, a table of results for each method on each metric, or graphs of performance over time. Key plots: path length vs. query number, accuracy vs. time, LLM tokens used vs. question index, distribution of edge weights over sessions, etc., which will illustrate Manny’s learning dynamics.

Experiment Structure

Experiment 1: Continual QA in Recipe Domain
	•	Setup: Start Manny with an empty graph. Provide 3 recipes (via /learn ingest= command, using LLM to parse each into a mini subgraph of ingredients-steps relationships). Then pose 5 queries sequentially (some factual: “What do I need for apple tart?”, some comparative: “Can I replace X with Y in apple tart?”, etc.). Halfway, provide a new recipe (pear tart) and then ask related questions.
	•	Baselines: (a) LLM-only: provide it the recipe texts and question each time (no memory of previous Qs except via prompt), (b) LLM+retrieval: embedding-based retrieval of relevant recipe text for LLM to answer, (c) Manny ablations.
	•	Metrics: Answer accuracy (we’ll craft known answers), number of LLM calls/tokens, path length improvements (if same or similar question asked twice), whether motifs are formed (e.g. common “tart” sub-recipe), time per query.
	•	Expectation: Manny should answer simple ingredient queries directly after one ingest. On substitution questions, Manny might need to traverse ingredient similarity edges (if not present, it might ask LLM or fail – we’ll see). If Manny reuses the “tart” motif for pear tart question, we should see a shorter path than apple tart initial answer. Baseline LLM will answer correctly but use more tokens and not get faster (and might not recall that user likes something if that was context). We verify Manny didn’t forget apple tart steps after adding pear tart (by asking again).
	•	Success criteria: Manny’s answers correct and /why shows logical path (e.g. apple tart → sugar → substitute honey for sugar). Motif “go_to_tart” is created and reused at least once (seen in logs). Token usage ~50% of baseline after the initial ingest.

Experiment 2: Evolving Knowledge Graph (Family relations)
	•	Setup: Construct a small family tree graph (could be fictional or real historical data). Stream in facts like “Alice is Bob’s mother”, “Bob married Carol”, etc., one by one. After each or every few facts, ask a query (like “Who is Alice’s daughter-in-law?” or “Who are Carol’s ancestors?”). After some additions, update a fact (like a divorce/remarriage scenario or a correction).
	•	Baselines: (a) a static graph query engine (like Cypher queries on the cumulative graph, which is basically cheating with full knowledge but no learning – we use this to get ground truth answers), (b) a GNN that is periodically retrained on the graph (we might not implement training due to small data, but we can conceptually compare if Manny’s answers match what an ideal graph algorithm would say).
	•	Metrics: Accuracy of relation queries (Manny should answer by finding path connecting the two people in question), explanation path vs actual relationship chain (should match exactly in a family graph). Check that if a relation changes, Manny doesn’t persist the old answer (catastrophic forgetting test: e.g., initially “X is Y’s spouse” then change to ex-spouse, Manny’s path should update). Measure edge count growth – ideally it equals number of facts input (plus maybe some for motifs like “is ancestor” which Manny might form by chaining parent links). If Manny uses motifs, maybe it identifies a recurring pattern “A is B’s ancestor if A-parent-…-parent-B chain”, which it could label or expedite. Transfer test: ask about a family not directly seen but similar structure (maybe add another family and ask a symmetric question to see if Manny’s motif for “ancestor” applied).
	•	Expectation: Manny should precisely reflect the data: its graph is the knowledge graph. So it should have 100% accuracy on queries that are answerable via the facts (basically it’s doing graph search). The main challenge is whether Manny’s curvature updates help or hinder here – since every fact might initially just be an edge of weight X, Manny might not need to update weights at all because all edges are equally important factual links. If Manny’s valence or usage updates skew the graph incorrectly (like over-emphasizing frequently asked relations vs rarely asked ones), we need to see if that biases answers. Ideally, Manny’s curvature mechanism will maybe make frequently traversed relation types slightly shorter, which is fine as long as correctness remains. We’ll verify Manny doesn’t “forget” a fact that wasn’t queried for a while (it might decay if not used – we must tune decay so factual edges remain unless explicitly removed).
	•	Success criteria: Perfect accuracy on all queries (since this is deterministic data). Manny’s /why path exactly corresponds to the actual relationship chain (we can automatically verify this against ground truth paths). No incorrect edges appear. After an update (like a changed relation), Manny’s answer changes accordingly (no lingering memory of old fact). Motif reuse: if the query “X’s granddaughter?” appears multiple times with different X, Manny might mine a motif “two-hop parent-parent = grandparent” and reuse it. We check motif count or edge reuse percentage for those patterns.

Experiment 3: Efficiency and Hardware Simulation
	•	Setup: Using one of the above scenarios (or a simple pathfinding problem), measure runtime performance. For instance, create a graph with N nodes and measure how long Manny takes to answer a query vs. a baseline that might re-compute from scratch. If we do a grid-world: generate a grid graph, do pathfinding 10 times with Manny updating weights vs. a standard A* running from scratch each time. Or measure how Manny’s query time scales with graph size (increase number of nodes gradually).
	•	Baselines: Non-learning graph search (for path tasks) or standard retrieval times (for QA tasks with vector DB). Possibly compare to a pure LLM approach’s latency (hard to compare since that includes network latency to API, but we can estimate).
	•	Metrics: Wall-clock time per query, averaged. We’ll use Python timers or time.perf_counter around Manny’s main loop. Count FLOPs or operations in a rough sense. If possible, run both Manny and baseline on the same machine for fairness. Also measure memory usage (Manny’s graph vs baseline’s data structures).
	•	Expectation: Manny might have overhead building the graph initially, but subsequent queries should be very fast (especially if using ANN for neighbors). The baseline LLM agent might be slower (due to thinking token by token). On hardware-friendly note, maybe simulate how many spikes would be generated (each edge traversal = 1 spike), and compare to how many MACs a transformer uses.
	•	Success criteria: Manny shows at least some improvement in either throughput or scaling exponent. E.g., if graph size doubles, Manny’s query time might increase linearly (due to ANN search ~ log N, plus path length maybe log N), whereas a transformer’s cost might increase more than linearly if prompt length grows. We want to demonstrate that with more knowledge loaded, Manny’s query latency doesn’t explode the way prompting an LLM with a huge context might. This would back the efficiency claim.
	•	(We understand 90 days might not allow thorough performance tuning, so this experiment is mainly to ensure Manny’s design has no obvious slowdowns; precise hardware gains might be more qualitative at this stage.)

Throughout these experiments, we will maintain a development checklist (see final section) to ensure all needed components (parsing, embedding, search, update, interface) are ready. Each experiment yields logs that we’ll analyze for evidence on H1–H5. We plan to allocate roughly: 3 weeks building MVP + unit tests, 2 weeks running Experiment 1 (tweaking Manny on QA domain), 1 week for Experiment 2, 1 week for analysis and Experiment 3, with contingency at the end.

Ablations and Failure Probes: In addition to normal runs, we design specific stress tests:
	•	To probe catastrophic forgetting: After training Manny on a bunch of info, deliberately feed a sequence of queries all about a new topic with conflicting use of common terms, then test an old query. E.g., teach Manny “Paris is capital of France” early, then focus on entirely different meaning of “Paris” (say Paris is a character in Greek myth) with high valence, and then ask capital of France – see if Manny still knows it or if its “Paris” node got repurposed wrongly. This tests the system’s ability to keep distinct contexts separate or represent polysemy (maybe Manny should have separate nodes for Paris(city) and Paris(person), and the LLM might help disambiguate on ingest).
	•	To probe motif overfitting: We can create a motif that is slightly wrong and see if Manny reuses it inappropriately. For instance, if Manny saved a motif from a pattern that doesn’t always apply, does it mistakenly apply it to a case where it shouldn’t? If yes, how to mitigate (maybe motifs should store context or have conditions). This is more of a manual inspection task due to time, but we’ll keep an eye out.
	•	To probe explanation degeneracy: Create a scenario where the shortest path is not the “real” explanation (e.g., in a knowledge graph full of irrelevant connections). See what Manny does – if it picks a spurious short path, that’s a problem. This might involve adding a few extra edges that connect disparate parts of the graph with slight weight, then asking a question that could travel through that edge but wouldn’t make sense logically. We check if Manny chooses the nonsense shortcut or a longer but correct path. If it chooses the nonsense because of curvature optimization, that indicates a need for logic constraints or valence gating.
	•	To probe LLM reliance: Try running Manny’s query answering with the LLM turned off after initial load. Does it still operate (maybe returning a graph path as the answer)? If Manny utterly fails without LLM, then H5 is not achieved. But if it can produce a structured answer (maybe just listing the nodes in the path), that’s at least something. We might do this in a controlled way: for some questions, force Manny to not call LLM for answer and see if using stored text or a template can answer basically. For factual queries, Manny can often just output the target node name as answer (which it knows from the graph). For explanation queries, Manny can output the node names in the path – not fluent but conveys info. The difference in quality versus with LLM phrasing will be noted.

All these experiments will produce a body of evidence. We’ll then compile results addressing each core hypothesis:
	•	H1 stable learning: e.g., “No significant drop in accuracy on earlier queries, curvature distribution remained bounded [plot], geodesic lengths reduced by avg 25% on repeats.”
	•	H2 interpretability: e.g., “Paths matched ground truth rationale in 9/10 cases in domain, human judges rated 90% explanations as clear.”
	•	H3 transfer: e.g., “Motifs mined = 4, reused in 6 later queries, new queries solved with 2× fewer examples than baseline needed.”
	•	H4 efficiency: e.g., “Manny used 40% fewer GPT-4 tokens and 30% less wall time than baseline agent over 20 queries,” perhaps extrapolate energy if possible (“projected 10× energy reduction if on neuromorphic, based on event counts”).
	•	H5 LLM usage: e.g., “After initial learning, Manny answered 70% of queries with only a final formatting call to LLM (no intermediate reasoning calls). In contrast, baseline agent invoked LLM reasoning ~5 times per query.”

These results will directly inform the feasibility assessment and our recommendations moving forward.

Economic & Hardware Outlook (12–24 Month)

Developing Manny involves costs in compute, engineering, and possibly special hardware. We consider the Total Cost of Ownership (TCO) for an MVP deployment and projected scaling, as well as the availability of suitable hardware in the next 1-2 years.

MVP Compute/Infrastructure Costs: In the short term, Manny can be developed and run on commodity hardware. The graph itself is not huge (in our tests, maybe tens of thousands of nodes). Storing embeddings (say 50k nodes * 768-dim float ≈ 150 MB) and edges (sparse) is easily handled in memory. The heavy component is the LLM calls. If using a paid API (like OpenAI GPT-4), each call costs $$$; however, because Manny aims to reduce calls, our usage might be moderate. For example, if Manny only calls the LLM for final answers and occasional ingestion, maybe we use a few thousand tokens per day per user. If integrated into an app with many users, this could add up. A cost estimate: suppose 1000 queries/day with 500 tokens of LLM each (for a fully LLM agent) vs Manny using 250 tokens each. At $0.03/1k tokens, baseline = $15/day, Manny = $7.5/day – about 50% saving, which is non-trivial. Over 90 days, that’s ~$675 vs $1350. So Manny could halve the variable LLM costs if scaled. If we consider an open-source LLM (running a smaller model on our own GPU), we’d have fixed hardware costs (GPUs, power). Manny’s advantage is potentially requiring a smaller model (maybe a 6B parameter model for embeddings and simple Q&A formatting, instead of a 175B model for complex reasoning). That drastically lowers memory and compute. Running a 6B model on one GPU is feasible, whereas GPT-4-level reasoning might need multiple GPUs or cloud instances.

Development labor and tools: Manny’s build requires an ML research engineer (graph algorithms, PyTorch or JAX skills maybe), an MLOps or dev to integrate LLM APIs and build UI, plus possibly domain experts for content if needed (like recipe knowledge, but that’s minor or can be synthetic). Over 90 days, that might be ~2 FTEs. Using mostly open-source libraries (for ANN, etc.) keeps cost low.

Hardware in 12–24 months: On the neuromorphic front, Intel Loihi 2 is currently accessible to research partners and expected to be more widely available within 1-2 years. Intel has an INRC (neuromorphic research community) one can join for cloud access to Loihi. If Manny shows promise, getting Loihi access to test could be next-step (cost: likely just collaboration overhead, Intel may provide boards or cloud time free for research, or dev kits if they start selling them might be a few thousand dollars). IBM TrueNorth was more a research prototype; IBM’s newer analog AI chips (like analog Phase-Change Memory arrays) might align but timeline unclear. SpiNNaker boards (SpiNNaker 2 especially) are available in academic circles and are essentially many-core ARM systems simulating spikes; one could purchase a SpiNNaker board (cost maybe tens of thousands?) but more likely use a hosted platform. Brainchip Akida is another neuromorphic chip commercially available now, optimized for spiking conv-nets – maybe less relevant for graph, but it shows neuromorphic is entering products (Akida used in some edge vision devices).

In optical computing, some startups (LightOn, Luminous, Lightmatter) aim to have optical accelerators that perform matrix multiplication or Ising machine solving by 2025-2026. Manny isn’t a direct matrix-heavy workload except possibly in the embedding search. But if we reframed Manny’s pathfinding as an optimization problem (like least cost flow), an optical Ising machine might solve small instances quickly. That’s speculative and likely not needed unless we go into extremely large graphs.

Mapping Manny to current hardware: Right now, Manny’s components run on CPU/RAM easily. The ANN search (HNSW) can use CPU or a bit of GPU. Graph traversal is pointer-heavy (CPU likely better unless graph is huge). The LLM usage is external API or a GPU if we host a model. A plausible production architecture now: Manny service runs on a CPU server with lots of RAM for the graph, and uses an API or local smaller GPU for the language model tasks. This is fine for moderate loads. If user count increases, we’d probably partition by user (each user or group of users has their own Manny graph perhaps) and scale out horizontally.

Where local-event models win on power: If Manny were deployed on an edge device (say a robot or a phone), a neuromorphic chip could allow continuous learning at very low power – something conventional DL struggles with. For example, an event-driven chip can run all day on a battery, whereas an equivalent GPU or CPU might drain it quickly if doing constant processing. So in domains like IoT or personal assistants that need on-device learning, Manny on neuromorphic hardware would be a game-changer. Within 24 months, we expect some neuromorphic development kits targeting IoT (Intel hinted at Loihi for robots/drones, and Brainchip markets Akida for embedded use). The path to hybrid deployment could be: use CPU/GPU now to validate algorithm, then port critical loops to neuromorphic. Perhaps initially run in a neuromorphic emulator (like Nengo or other frameworks that simulate spiking networks) to ensure compatibility. If Manny’s operations can be expressed as spiking rules (which they can, conceptually: e.g., use spike trains to represent distance, STDP for weight update with a modulatory signal for valence), then when hardware matures, transferring should be straightforward. We’ll keep the code modular (perhaps separate classes for “GraphStore” and “ThreadRunner”) so we can swap out implementations – e.g., a ThreadRunner that uses Loihi’s API.

Memory and Storage costs: Manny’s knowledge base grows with usage but prunes and compresses. We anticipate mostly storing symbolic nodes (concept labels) and their embeddings, plus edges. Even a large knowledge graph like ConceptNet has 34k concepts and ~100k edges, which is peanuts for modern memory. If Manny were extended to something like full Wikipedia scale, we’d have millions of nodes – that would be a big data engineering challenge (though still possible: millions of 768-d vectors is a few GB; edges maybe tens of millions – might need graph database tech or specialized store). But in next 24 months, if Manny concept proves itself, we could integrate with scalable graph databases or vector DBs. Many exist (Neo4j, RedisGraph, Faiss, etc.). So horizontally scaling Manny’s knowledge is more a software challenge than hardware.

Cost/Benefit Analysis: Over 12–24 months, if Manny is pursued, the costs will mainly be R&D staff and cloud compute for LLM. But if Manny indeed reduces LLM calls by >50%, and if LLM usage is a big cost driver, Manny could save a lot of money in production. For instance, if an enterprise QA system costs $100k/year in OpenAI API calls, Manny might cut that to $50k. That easily justifies investment in development. Also, Manny’s continual learning means less need for frequent fine-tune training runs or prompt engineering updates, which is a cost saving in engineering maintenance. On the hardware side, adopting neuromorphic chips when available could lower operational power costs if that’s relevant (for cloud, power is cost; for edge, battery life is value). Right now neuromorphic is not something you can just buy off the shelf in volume, so we wouldn’t count on it in near-term production. But being ready for it is good future-proofing. Perhaps in 2 years, Intel might have a Loihi-based accelerator card or cloud service you can pay for – if Manny is alive then, it could take advantage.

Resource Needs in Near Term: a beefy CPU and maybe a modest GPU for dev. Possibly multiple instances if testing concurrency or multi-user. If the user base or data gets large, a distributed memory graph might be needed, but that’s beyond MVP. One risk is if Manny were to ingest a ton of data, the graph might become huge – then memory and search time grow. But if that’s due to success (people want it to have more knowledge), we can invest in scaling solutions (e.g. sharding graph by topic, etc.). It’s similar to how vector DBs scale by sharding embeddings; we could do the same.

In summary, next 12 months: use existing hardware (cloud CPUs/GPUs), optimize algorithms (maybe parallelize parts on standard hardware). 12–24 months: be ready to integrate specialized hardware if it becomes practical – likely via partnerships or research grants. The cost trajectory looks favorable: Manny’s approach, if it works, saves on the most expensive part of AI systems currently (LLM calls) by trading it for cheaper graph computations. Graph ops are memory-bound but memory is cheap relative to massive model inference. And if extended, neuromorphic chips promise to drastically cut the cost of continuous learning computations. We’ll keep monitoring hardware announcements. A realistic expectation is we could have a Manny running on a Loihi 2 emulator within a year (perhaps demonstrating spiking version of one of our experiments). Within 2 years, maybe actual hardware for small-scale use. For large-scale (millions of nodes), neuromorphic may need more time, but we can cluster many chips (SpiNNaker approach).

Finally, we note that implementing Manny doesn’t lock us out of using future LLM improvements – it’s complementary. If LLM inference cost drops (cheaper models, distillation), Manny benefits anyway by being able to call them more if needed. And if some new form of memory-LLM hybrid model appears, we could incorporate it. So economically, Manny development is a hedge: if LLM stays expensive, Manny saves money; if LLM gets cheap, Manny is still a useful architecture for interpretability and adaptability.

Ethics & Safety Considerations

Building Manny Manifolds comes with several ethical and safety dimensions to address, particularly because it is an AI system that learns from user interactions and provides reasoning traces. We focus on interpretability, user understanding, data governance, and alignment with user intent/values.

Interpretability as a Safety Feature: Manny’s design is motivated by the desire for transparent reasoning – this is inherently a plus for safety. By giving users a “window” into why it answered a certain way, Manny can help build appropriate trust. Users (or developers) can spot if Manny’s reasoning is going off-track or based on dubious connections. For example, if Manny answers a medical query and the path shows it connected two unrelated symptoms due to a coincidental graph link, a doctor can catch that and not trust the answer. This is far better than a black-box LLM that might hallucinate a convincing but wrong answer with no explanation. We will ensure the explainer UI is clear and user-friendly: perhaps visualizing nodes as concepts and edges as arrows with tooltips for curvature/valence. We’ll include an “uncertainty” indicator – e.g., if the shortest path found was only slightly better than the next, show low confidence, or if the curvatures on path are low (meaning Manny isn’t strongly certain about those links), signal that. This could be as simple as a warning icon or a confidence score next to the answer. Ethically, communicating uncertainty helps prevent misuse of Manny’s outputs. Users should be told “This is Manny’s current best guess path, but it’s not very confident” in some cases.

Alignment and Mislearning: Because Manny learns from user input (and possibly from LLM suggestions), it could pick up biases or incorrect associations. For instance, if a user implicitly reinforces a stereotype (intentionally or not), Manny’s valence mechanism might increase curvature on an edge representing that stereotype (e.g., linking a demographic to a negative trait). This is a risk – Manny could essentially learn biases from usage. Mitigation: incorporate guardrails and oversight on what gets learned. We might enforce that certain sensitive relationships cannot get high curvature without extra confirmation (like requiring multiple sources). Also, Manny’s valence channel can be double-edged – it allows user to emphasize or deemphasize info, but a malicious user or even a biased LLM ingest could set wrong valences. We’ll implement valence constraints: e.g., cap how strongly negative a single user’s valence feedback can make an edge without moderation. Perhaps treat valence carefully on protected attributes (if domain has them). If Manny is used in a multi-user setting, we’d have to have content moderation on inputs that feed Manny’s learning (e.g., if someone states false info, do we allow Manny to ingest it as fact?). Likely in MVP we assume a single trusted user or curated content. In future, could have a veracity score or source for each edge in the graph, and Manny could forget or mark uncertain any info that isn’t from a reliable source.

Persuasion and Valence Exploitation: Another angle – Manny’s valence mechanism effectively lets it attach an “emotional” or importance weight to edges. Could this be manipulated? If an adversary had access, they could potentially set valence=1 on misinformation edges to bias Manny’s answers. In a user context, Manny might overly trust information delivered in a very confident or positive way (since valence might be set high for it). This is somewhat analogous to prompt injection but on the knowledge graph. Mitigation: restrict who or what can set high-valence information. We might decide that only the user (not the LLM) can set valence, or that by default ingestion from external sources comes with neutral valence, and only user explicitly using /valence command changes it. Also, we can implement valence decay for things that haven’t proven useful – if something was hyped (high valence) but never actually leads to good outcomes, slowly reduce its effect.

Data Provenance and Privacy: Manny is a form of knowledge base, so we must handle data responsibly. If Manny ingests documents, we should keep track of sources for each edge or node. This helps with provenance – if asked, Manny could say “I learned this from [source]”. In the MVP, we can store a reference in each node/edge object to where it came from (URL, user, etc.). Ethically, this is good for attribution (especially if Manny uses any text from sources, though currently it’s storing embeddings/relations, not full text aside from labels). Privacy: if Manny is used with personal data (like a personal assistant learning user habits), it will accumulate personal info. We should implement a “forget” mechanism – e.g. the user can command Manny to remove certain nodes/edges or whole subgraphs (like “forget this conversation” or GDPR-style erasure). With a graph, forgetting is more straightforward than in a giant neural net: just delete the corresponding nodes/edges and any motifs involving them. We’ll include a /reset or selective forget command in the interface (the README shows /reset clears session but not manifold, maybe we extend to clear manifold parts).

Misuse scenarios: If Manny were used in an open domain (like internet knowledge), one must be cautious that it could connect some innocuous concepts in dangerous ways (like someone could intentionally feed it partial info that leads it to an incorrect conclusion). Because Manny’s reasoning is explicit, malicious actors might also try to game it: e.g., if Manny were answering questions on a forum, someone could read its explanation path and find a way to insert a false edge that Manny would follow next time. This is an exotic threat, but basically, exposing reasoning could expose the attack surface (the attacker sees exactly which link Manny relied on, and might target that link with misinformation injection). To mitigate, one could require confirmation or trusted sources for updates to critical edges. Also, monitor for feedback loops: Manny might strengthen an edge because it used it, and used it because it was strengthened – a form of self-reinforcement that might entrench mistakes. We can guard by having an external correctness check (maybe the LLM can double-check an answer after the fact; if even the LLM says “that doesn’t look right,” maybe reduce that curvature instead of reinforce). This is akin to adversarial training: use the LLM to sanity-check Manny’s new knowledge occasionally.

User Agency and Understanding: We need to ensure users (especially non-experts) can understand Manny’s outputs. While we champion interpretability, showing a raw graph path with jargon nodes might confuse some. We should design the explanation format to be digestible: e.g., convert the path into a natural language explanation using the LLM (“I believe X because X is related to Y which causes Z…”). This is a double-edged sword – that reintroduces an LLM in explanation, possibly generating a story not exactly matching the precise graph. We can do both: a visual graph plus a short natural summary. And allow the user to drill down if they want. Ethically, we should not overwhelm or mislead the user with the explanation; it should aid their decision making, not require them to debug Manny themselves.

Failure Logging and Red-Teaming: We will implement robust logging of Manny’s decisions, especially when it’s unsure or gets something wrong. These logs (with user permission and anonymized if needed) will be reviewed to identify failure modes. For example, if Manny’s path consistently ignores a certain type of edge, maybe there’s a bias in how curvature updates work. We’ll also do some red-team testing ourselves: try to trick Manny with input that could create bad associations (like see if we can make Manny conclude something harmful by feeding it a series of biased statements). Since Manny is relatively small-scale, we can correct issues as we find them by adjusting rules or adding guardrails (like “if concept = [sensitive topic], require direct source edge rather than transitive inference”). If Manny were deployed in a domain like medical or legal, extra precautions are needed: ensuring that sources are authoritative and maybe each answer path includes a vetted reference. Manny could actually assist with that by linking to the source node content.

Guardrails against Persuasion: If Manny were used interactively (like a chatbot), one might worry about it persuading or manipulating via valence or path emphasis. Because Manny itself is not a generator of persuasive language (the LLM would handle tone), and because its reasoning is exposed, it’s less likely to engage in covert manipulation. But imagine Manny decided one concept is very “valuable” and always steers conversation there (like it gets a high-curvature hobby node and always brings it up). That could annoy or influence the user. We can detect if Manny’s answers are overly repetitive or obsessive about a node (e.g. if one node is on an unusually high fraction of answer paths, that’s a red flag unless justified). Mitigation: ensure diversity by decaying edges that are overused in answers without actual user interest (maybe measure if user is satisfied with those answers or not).

Security of the System: Manny’s knowledge graph should be protected from unauthorized access or tampering, as it effectively stores what the AI “knows.” Use encryption or at least proper auth if multi-user. The MVP being local or with one user is fine.

Transparency and Consent: If Manny uses an LLM or external data, disclose it to users. And if Manny learns from user input, implicitly it might store some of that – users should know their inputs can be remembered (with ability to clear them as mentioned). If multiple people’s data is in one graph, that’s a privacy issue – likely we’d maintain separate graphs per user or a shared one only with consent and for communal benefits (like a public wiki-bot version of Manny).

Bias and Fairness: Manny’s outputs depend on what it learns. If initial knowledge or user interactions are biased, Manny could produce biased results. However, thanks to interpretability, such biases might be easier to spot (the path would reveal if Manny connected, say, a demographic node to a crime node). We commit to actively monitoring for biased associations in the graph and correcting them. Possibly incorporate a bias-check step: have a list of known undesirable associations and either prevent them from forming or flag them. The LLM or a separate module can scan the graph periodically for edges that might reflect stereotype (there’s research on knowledge graph bias detection we could leverage).

Scope of Use: Manny might be more suitable in domains where correctness and explainability are crucial (e.g. medicine, law, finance). But those also require rigorous correctness. Manny’s advantage is it can incorporate verified knowledge (like medical guidelines) directly and show reasoning. During MVP, we stick to safe domains (cooking, fictional families) to not risk harm. If applying to a real sensitive domain, we’d involve experts and ensure thorough validation. The idea is Manny shouldn’t guess when it doesn’t know – if the graph has no path, it should say “I don’t have information on that” rather than hallucinate. We’ll implement a threshold: if the distance to answer is above some cutoff (meaning the graph is very tenuously connecting the question), better to abstain or ask for more info. This addresses the alignment principle of not producing info beyond its knowledge.

In conclusion, ethical design for Manny means:
	•	Transparency: Always show or offer the reasoning path, and origin of knowledge.
	•	User Control: Provide controls like feedback (/valence) and forgetting, and ensure user can correct Manny easily (like if Manny got something wrong, a command to weaken that edge or mark it incorrect).
	•	Prevent misuse: Monitor learning to avoid entrenching bias or falsehoods, use human oversight on critical updates.
	•	Data protection: Keep personal data safe and allow deletion.
	•	Honesty: Manny should indicate when it’s uncertain or when its knowledge might be incomplete, rather than overstate confidence.

By adhering to these, we hope Manny will be not just an effective system, but a trustworthy one. The MVP will mostly involve internal evaluation, but we will treat even synthetic use with these principles (e.g., logging all assumptions, separating any use of external sources to ensure attribution in our report). As we move beyond MVP, a more formal ethical assessment would be done especially if targeting real user data.

Plan on a Page (90-Day Roadmap)

Goal: Develop a working Manny Manifold MVP, validate core hypotheses (H1–H5) on small-scale tasks, and decide go/no-go for further development.

Team & Roles:
	•	Project Lead/Researcher (Owner: H1–H3 validity, overall integration)
	•	ML Engineer (Owner: algorithm implementation, H4 performance)
	•	NLP/LLM Engineer (Owner: LLM integration, H5, prompt engineering)
	•	DevOps/Data Engineer (Owner: infrastructure, experiment pipeline, data management)
	•	*Advisor/Domain Expert (as needed for evaluation design, e.g., a UX person for interpretability feedback, domain expert for testing correctness)

(In a small team, one person may wear multiple hats. Initially, assume 2-3 people collaborating.)

Infrastructure/Tools: Python stack with potentially PyTorch (for embeddings), NetworkX or custom for graph, FAISS or HNSWLib for ANN, OpenAI API or HuggingFace for LLM. Use GitHub for version control (we have the repo). Possibly use Weights & Biases or similar for logging experiment metrics and plots.

Timeline:
	•	Week 1–2: Setup & Core Engine Build
	•	Implement Manifold data structures (Node, Edge with curvature, etc.) and basic operations (add node, add edge, query neighbor, etc.). Owner: ML Engineer.
	•	Integrate embedding model (use open-source like SentenceTransformer) for creating node vectors, and ANN index for nearest neighbor search. Owner: NLP Engineer.
	•	Implement thread runner: given a start and goal (from user query parsed), find a path (possibly greedy or A* like using negative curvature as negative cost) through the graph. Owner: ML Eng.
	•	Implement plasticity update function (curvature Δκ rule) and ensure it’s called on traversed edges. Owner: ML Eng.
	•	Set up basic CLI loop or notebook function to simulate input-> Manny output. Possibly reuse CLI from repo ￼, adapting as needed.
	•	Milestone end of Week 2: Can ingest a small graph and answer a simple path query with /why showing the path and updated curvature values. e.g., feed “A->B, B->C” relations, ask A->C, see path A-B-C with curvatures updated. Verified basic learning (if ask again, maybe curvature made path same but that’s fine).
	•	Week 3: LLM Integration & QA Capabilities
	•	Incorporate an LLM for two tasks: (1) parsing/ingesting knowledge (e.g., user provides a text or a command like /learn domain, LLM extracts key nodes/edges), (2) generating final answer sentences or summarizing explanation. Owner: NLP Engineer.
	•	Develop prompt templates for ingestion (e.g., “extract key facts from this text into triples”) and for answer explanation (“given nodes and path, generate explanation in English”).
	•	Test with a known small text (like a short recipe) to see if LLM correctly populates Manny’s graph. Possibly refine prompt or provide exemplars.
	•	Put in safeguards: LLM output validation (e.g., if it says X->Y relation, make sure X,Y are in text).
	•	Milestone: Manny can take a paragraph input, populate graph, and answer a factual question from it by finding a path between relevant entities, then produce an English answer. For example, ingest “Alice is Bob’s sister. Bob is Charlie’s father.” Ask “How is Alice related to Charlie?” Manny finds Alice->Bob->Charlie path, then LLM phrasing: “Alice is Charlie’s aunt (because Alice is Bob’s sister and Bob is Charlie’s father).” This demonstrates end-to-end loop working with interpretability.
	•	Week 4: Internal Testing & Refinement
	•	Run initial ablations to ensure stability: feed some sequences of inputs, see if any divergence or weird graph growth. Tweak parameters (η, decay) as needed. Possibly implement the monitor to throttle η if needed. Owner: ML Engineer.
	•	Ensure /why output is clear. Might refine viz.py ￼ to print edges with concept names and Δκ. Possibly create a small visualization (ASCII or simple network diagram using a library) for explanation. Owner: Project Lead/UX.
	•	Prepare synthetic data for experiments: e.g., write 5 small recipes and queries, define expected answers. Also prepare a family tree example data. Owner: Domain Expert (or team collectively).
	•	Milestone: All building blocks are in place, ready to conduct planned experiments. Code is documented, and we have a plan for metrics collection (maybe functions to compute metrics from Manny’s log or state).
	•	Week 5–6: Experiment 1 (Recipe domain continual QA)
	•	Load or script the recipe interactions: simulate ingest and user questions in sequence. Automate Manny running through them. Owner: ML Engineer.
	•	Run baseline: possibly a simple retrieval+LLM script for same questions (we might use GPT-4 directly to answer based on provided recipes). Owner: NLP Eng.
	•	Collect data: For each question, log Manny’s answer, path, tokens used, time. Do same for baseline. Owner: DevOps Eng (set up logging).
	•	Evaluate correctness (manually if needed or by checking if key answer phrases appear). For explanation alignment, compare Manny’s path to a gold rationale (which we write up for each question). If we have a second person, have them blind-rate the explanations. Owner: Project Lead.
	•	Adjust as needed: If Manny fails some, see why (did LLM parse incorrectly? Did Manny not link nodes? Fix knowledge ingestion or maybe manually add edges to mimic a user teaching Manny gradually). This is an iterative tuning – e.g., we realize Manny didn’t know that “sugar” and “honey” are similar, maybe we need a small semantic link injection or call LLM to find substitutes. We might incorporate a small WordNet or use LLM to add an edge “honey ~ sugar (similar)” with low curvature initially.
	•	Milestone end of Week 6: Results for Experiment 1 compiled. We expect to have a table: question, baseline answer & tokens, Manny answer & tokens, correctness, explanation alignment score. Also an observation: did Manny improve on repeats? (We intentionally include a repeat or variant question). If not, check if Δκ logic is effective; adjust formula or budget if needed.
	•	Week 7: Experiment 2 (Knowledge Graph evolving)
	•	Implement a simple pipeline to feed Manny a sequence of triples (or sentences that the LLM will convert to triples) and queries. Could be done via script or manually step by step. Owner: ML Engineer.
	•	Possibly disable LLM for ingestion here and feed structured triples directly (to isolate Manny’s learning on structured data). We can do both: one mode where LLM reads a sentence like “X is Y’s mother” and Manny adds edge (X ->child-> Y or Y->parent-> X accordingly). That might require LLM for relation extraction anyway unless we pre-format.
	•	Query Manny after certain additions and changes. Record accuracy and path.
	•	Run baseline: since it’s small, baseline could be just a ground truth oracle (we know the answer from our own data). Or use a neo4j query. Not much compute baseline needed because Manny should be exact here.
	•	Evaluate if Manny maintained all facts and adapted to changes. Check metrics: any forgotten relations (if curvature decayed an unused edge to zero maybe?). If yes, adjust decay or ensure once-ingested factual edges don’t fully decay (maybe set a floor on decay for edges that came from authoritative ingest vs ephemeral edges created via reasoning?). We might implement such a rule if needed.
	•	Milestone: Results for Experiment 2: Manny answered all queries correctly (target), or note any misses. Explanation paths exactly matched expected relationship chains. If something went wrong (like Manny took a shortcut through an unrelated connection), note it and consider adjusting the search heuristic to prefer known relation types perhaps. But likely in a family tree, it’ll be fine.
	•	Check H3 evidence: if we asked multiple ancestry queries, did Manny form a motif like “grandparent path” and label it? If not automatically, perhaps user can manually save one to see the mechanism. If Manny didn’t auto-detect motifs, maybe the threshold wasn’t met (not enough usage). We could force a scenario where a motif would form (like lots of queries that share subpaths). If motif mining isn’t clearly shown, it’s okay – we can still conceptually argue it, but a demo of at least one motif reuse would be nice. If needed, tweak motif mining threshold to catch a pattern we know is there.
	•	Week 8: Synthesis & Additional Experiments
	•	By now, we revisit hypotheses: see if any are not adequately tested. For example, if H4 (efficiency) hasn’t been measured because our data is too small to see gains, we might artificially scale something to measure it. Maybe create a larger synthetic graph and time Manny vs some baseline for a query. Or measure difference in token count clearly on a longer session.
	•	Run the efficiency test: e.g., 20 questions with baseline agent vs Manny, measure cumulative tokens and time. We might simulate the baseline agent’s chain-of-thought or simply use GPT-4 with few-shot for each question (less systematic though). At least measure relative API usage and maybe simulate if an LLM call takes X seconds vs Manny’s local ms-scale operations.
	•	If available, try running Manny on a low-power device (maybe a Raspberry Pi or a laptop unplugged) to qualitatively see speed and if possible measure battery usage for an hour of queries vs an LLM approach (which likely calls cloud anyway so not directly measurable). This is optional.
	•	Evaluate H5 explicitly: Count LLM calls/tokens Manny made in Experiment 1 and 2. Check if it’s about half or less than a naive approach. If not, identify why (maybe ingestion calls were large). If Manny still used a lot of LLM, consider ways to reduce (like caching embeddings or re-using the same LLM output for multiple edges, etc.) – but major changes likely out of scope now. We’ll report whatever we got.
	•	Summarize each hypothesis with evidence from experiments: e.g., H1 supported by no forgetting in Exp2 and path shortening in Exp1, etc. Owner: Project Lead.
	•	By end of Week 8, have a draft of findings and a decision: does MVP show enough promise (likely yes unless major instability was unresolved)?
	•	Week 9: Documentation & Risk Assessment
	•	Compile all results, graphs, logs into a coherent report (which would form the basis of something like this dossier’s findings section). Owner: Project Lead.
	•	Re-assess the risk register with actual data: e.g., did plasticity tuning end up okay or did we have to hack it? Did explanations hold up in user feedback? Update risk severities if needed.
	•	Formulate recommendation (go/no-go) based on results. Perhaps engage a stakeholder review meeting to present MVP outcomes.
	•	Also document how one would extend the MVP (what next tasks or domains, what engineering to harden it, etc.).
	•	Week 10–12: Polish & Stretch Goals
	•	If all went well early, use last weeks to address any remaining stretch: e.g., make a nicer UI demo (maybe a small web app interface to converse with Manny). This is good for showcasing interpretability: could use a library to draw the graph path live. Owner: DevOps/UX.
	•	Conduct a red-team test: have a couple team members deliberately try to confuse Manny or push it to failure in a controlled environment, note issues. This might have been done informally earlier too. Document these to include in the “limitations” or red-team critique section.
	•	If hardware access is available (maybe an invitation to run something on Loihi via a partner), try mapping a small piece (like run the graph update rule as STDP on a spike sim). If not, possibly just outline how it would be done.
	•	Ensure all code is cleaned up, with appropriate modularity (so if continuing, it’s maintainable). Possibly write unit tests for critical functions (e.g., curvature update, motif extraction) to catch regressions.
	•	Milestone end of Week 12: MVP deliverables ready: final report of results (like this dossier), a demonstration of Manny’s capabilities (scripted or live), and a decision from stakeholders on proceeding.

Post-MVP Next Steps (if Go): These aren’t in 90-day scope but would be planned at high level:
	•	Increase scale (bigger knowledge, more diverse domains),
	•	Multi-user or cloud service architecture (with memory per user),
	•	More robust motif learning algorithms (maybe graph mining algorithms),
	•	Integration with knowledge base or databases (to import large graphs initially),
	•	Safety module (monitor graph for bias, implement user feedback UI for corrections),
	•	Begin hardware prototyping (collaborate with neuromorphic lab to deploy key loops on chip).

The above roadmap keeps the focus: achieve a functioning prototype and gather evidence for Manny’s value. It balances development and evaluation such that by around week 8 we have concrete evidence to inform a go/no-go. The timeline includes buffer (we allocated 12 weeks, which is ~90 days, leaving some slack or iteration time in between tasks).

All tasks have clear owners, but in a small team, people collaborate. For instance, the ML engineer and NLP engineer work together on ingestion (graph + LLM). Owners ensure tasks don’t fall through cracks.

We will track progress with a Kanban (backlog: engine, LLM integration, experiments; in-progress; done). Weekly check-ins to adjust if something takes longer (for example, if the LLM parsing is error-prone, we might spend extra time or simplify domain).

Success by Day 90 means we have demonstrated at least one scenario where Manny learns incrementally, provides correct answers with explanations, reuses learned knowledge, and uses fewer LLM resources than an alternative. That sets the stage for a decision to invest further (or not, if results were underwhelming or complexity too high).

MVP Build Checklist

To ensure we achieve the above, here is a checklist of components and tasks required for the MVP, categorized by function:
	•	Data & Domain Setup:
	•	Define small test domains (e.g., Recipe dataset with ~5 recipes and Q&A, Family relations dataset).
	•	Prepare any necessary ground truth answers and explanation outlines for evaluation.
	•	If using an LLM, obtain API access or set up local model (and ensure it’s working).
	•	Core Code Modules:
	•	Graph Manifold Implementation: Node and Edge classes with attributes (embedding, curvature, neighbors, motif id, etc.). Manifold class with methods: add_node(token), add_edge(u,v,weight), get_edge(u,v).
	•	Embedding & Similarity: Function to generate embedding for text token (using pre-trained model). Cosine distance function for similarity.
	•	ANN Index: Integration of HNSW or Faiss for nearest neighbor search to find closest node for a given embedding (for mapping user query words to existing nodes, etc.).
	•	Thread Runner (Pathfinding): Implement algorithm to find a path between start and goal nodes. Could be greedy BFS that accounts for curvature (like cost = distance - α*curvature). Possibly parameter α if needed to balance embedding distance vs curvature (the engine in repo likely has some approach). Ensure it can return the path found and maybe alternative path if needed.
	•	Plasticity Update: Implement update_on_traverse(manifold, path, valence) as per plasticity.py ￼. Include the per-turn budget and L1 shrink steps. Make sure to clamp values and update edge/node curvature accordingly. Return info (like avg Δκ) for logging.
	•	Valence Handling: Store a default valence (set by user or context) for the next input. Provide a command or function to set valence. Ensure the thread runner passes this valence to update function.
	•	Consolidation (/sleep): Implement or stub basic consolidation: e.g., call micro_decay on active region, prune edges below thresholds, and perform motif mining.
	•	Motif Mining: Implement at least a simple approach: e.g., go through recent threads and if a subpath appears frequently, record it as motif (Motif class with path_signature). Maybe increment reuse_count on edges used that are marked motif.
	•	Pruning: Remove edges with very low |κ| and low weight if needed to maintain sparsity target. This should be careful not to drop important edges.
	•	Ensure state saving/loading if needed (not essential for MVP, but good to have /save_state, /load_state as in README).
	•	LLM Integration:
	•	Ingest Knowledge (/learn command): Implement pipeline to ingest either a text document or knowledge domain. For text: call LLM (or use a simplistic parser for known pattern sentences) to extract key triples or relations. Then add nodes/edges to graph. Possibly nudge curvature for edges if some are more certain (like primer knowledge).
	•	Query Parsing: If user query is in natural language (“How do I do X?”), decide how to interpret in graph terms. Possibly identify the topic node (“X”) and goal (maybe a type of answer needed). For now, could simplify: assume queries directly name concepts present in graph or to be added. For more complex questions, might need LLM to break it down. At MVP, we might restrict query to simple forms (“What is relation between A and B” or “Find a path to X under condition Y”).
	•	Answer Generation: Given the result of thread traversal (which could be a target node or a sequence of nodes in path), produce a human-friendly answer. If the answer is just a node label, maybe just return that. If explanation requested, either output the raw path or feed path into LLM prompt to get a narrative explanation. Ensure not to hallucinate beyond path: perhaps list the relationships in path for LLM to incorporate.
	•	Safety for LLM: If using a public API, include user/system prompts to avoid it going off-track (like instruct it to output in a specific format for parsing, etc.).
	•	Interface & Commands:
	•	Implement command parser for: normal question, /why (explain last answer), /map [topic] (show local neighborhood of a node, nice for debugging), /valence [value] (set valence), /save [name] (save last path as motif, if not auto done), /sleep (trigger consolidation), /reset (clear session or graph), possibly /quit.
	•	Logging within commands: ensure each step prints or records relevant info (like after /why, show Δκ on edges).
	•	(Optional) Simple front-end: remain in CLI for MVP, but ensure outputs are clearly formatted (maybe indent explanation, etc.). If time, a minimal web UI using Flask or Streamlit to demonstrate Manny answering with a graph visualization.
	•	Metrics & Evaluation Tools:
	•	Instrument the code to collect metrics: e.g., count LLM tokens per call (if API gives it), accumulate in a counter. Time each query handling (Python time).
	•	Add hooks to record path length, maybe curvature stats each turn (like mean curvature, number of edges) to track stability and growth.
	•	Store history of Q&A and motifs usage so that after running a scenario, we can analyze reuse counts, etc.
	•	Implement evaluation scripts: e.g., a script to run through the recipe scenario automatically and output logs, similarly for family scenario. This can output a JSON or CSV of interactions with fields (question, Manny_answer, Manny_path, baseline_answer, correct, tokens_used, etc.) for analysis.
	•	Prepare any manual evaluation forms needed (for explanation alignment, we might just visually compare since small data).
	•	Sanity test these tools on a mini-run.
	•	Dashboard/Visualization:
	•	(Optional) Use a Jupyter notebook or W&B to plot metrics (like token usage over questions, path length over repeats).
	•	(Optional) Graph visualization for understanding Manny’s internal state (maybe generate a PNG of the graph after learning, to see if structure looks sensible).
	•	Testing & QA:
	•	Unit tests for critical functions: e.g., feed a known small graph into thread runner, ensure it finds correct path; test plasticity update on a simple edge (increase usage, curvature should increase but clamp properly; test sign with positive/negative valence).
	•	Simulate extreme inputs: lots of valence=1 updates to one edge – does it clamp at 1.5? if not, fix logic.
	•	Check memory management: after resets or prunes, ensure no ghost references.
	•	Check multi-turn consistency: ask same question twice, answer should be at least as good second time (if worse, investigate).
	•	Test with valence negative case: if we mark something valence -1 and traverse, curvature should decrease (weaken connection). Confirm that reflecting in later answer selection (maybe Manny will avoid that path next time).
	•	Documentation & Usage Guide:
	•	Write a README or usage notes explaining how to run Manny (for ourselves or any reviewer).
	•	Document assumptions/limitations in code comments (e.g., “we assume one concept per node token, no multi-word synonyms unless manually handled”).
	•	If delivering this to others, provide example transcripts of Manny working to illustrate capabilities.
	•	Ethical Checks:
	•	Ensure no sensitive personal data is present in test knowledge (we use fictional or public info).
	•	If LLM is used, follow its usage policies (e.g., avoid sending disallowed content, though our domain is benign).
	•	The system should say “I don’t know” or fail gracefully if it truly has no path (maybe our thread runner can detect if goal unreachable and return a special token which LLM then turns into “I’m sorry, I haven’t learned about that yet.”).
	•	Bias check: insert a test where Manny could make a biased leap (maybe an unfair association) and ensure either Manny doesn’t or we handle it. This might be as simple as verifying Manny’s knowledge remains literally what fed (since we didn’t feed it biased content, it should be fine).
	•	Go/No-Go Criteria Validation:
	•	Collate evidence from experiments and ensure each hypothesis has at least one supporting data point or observation. If something is lacking (say motif reuse didn’t clearly emerge due to limited queries), consider running an extra loop to demonstrate it.
	•	Summarize results in a concise form (for an exec summary). E.g., “In recipe domain, Manny answered 90% questions correctly vs 85% for baseline and used 60% fewer tokens, with 2 key reasoning paths reused later. No forgetting observed over 10 questions.”
	•	Draft the final recommendation and next steps plan.

Each checklist item will be ticked off as we proceed. Many are already completed in concept by referencing code (like plasticity from repo), but we’ll verify in our integrated MVP.

By following this checklist, we cover all necessary pieces for a robust MVP demonstration and analysis, thus maximizing the chances of a confident “Go” decision with evidence in hand.